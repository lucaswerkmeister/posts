<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Lucas’ Posts</title><link>https://lucaswerkmeister.de/posts/</link><description>I suppose this is a blog of sorts – or at least a place where I occasionally post stuff. Not necessarily about anything in particular.</description><lastBuildDate>Tue, 12 Aug 2025 19:23:44 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Falsehoods Wikimedians Believe About Gadgets</title><link>https://lucaswerkmeister.de/posts/2025/08/12/falsehoods-wikimedians-believe-about-gadgets/</link><description>&lt;article&gt;

&lt;p&gt;
&lt;a href="https://www.mediawiki.org/wiki/Extension:Gadgets"&gt;Gadgets&lt;/a&gt; are a popular way to improve the user experience on MediaWiki wikis,
        including &lt;a href="https://meta.wikimedia.org/wiki/Gadgets"&gt;Wikimedia wikis&lt;/a&gt;.
        This humorous list contains misconceptions that some of us, consciously or unconsciously,
        may hold about various aspects of gadgets,
        based on my own experience as an interface administrator on Wikimedia Commons.
      &lt;/p&gt;
&lt;p&gt;
        For some of the falsehoods, I’ve included some extra text that you can read if you like
        (click the headings to collapse/expand them&lt;script src="orExpandCollapseAll.js"&gt;&lt;/script&gt;).
        But I’d suggest first reading through the whole list once,
        as I think the text flows better this way :)
      &lt;/p&gt;
&lt;p&gt;
        (With thanks to Ash_Crow, Sukkoria and Brooke Vibber for proofreading and suggestions!)
      &lt;/p&gt;
&lt;h2&gt;The list&lt;/h2&gt;
&lt;h3 id="1"&gt;1. The user who created this gadget is its maintainer&lt;/h3&gt;
&lt;h3 id="2"&gt;2. The user who most often edited this gadget is its maintainer&lt;/h3&gt;
&lt;h3 id="3"&gt;3. The user who last edited this gadget is its maintainer&lt;/h3&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;h3 id="4"&gt;4. This gadget’s maintainer uses it regularly&lt;/h3&gt;
&lt;/summary&gt;
&lt;p&gt;I maintain &lt;a href="https://commons.wikimedia.org/wiki/Help:Gadget-ACDC"&gt;AC/DC&lt;/a&gt;, but use it &lt;a href="https://commons.wikimedia.org/w/index.php?title=Special:Contributions/Lucas_Werkmeister&amp;amp;tagfilter=ACDC&amp;amp;limit=500"&gt;only sporadically&lt;/a&gt;.&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;h3 id="5"&gt;5. This gadget’s maintainer is able to edit its code&lt;/h3&gt;
&lt;/summary&gt;
&lt;p&gt;
          If a gadget started out as a user script, the author of the user script may not have the permissions required to edit it after the conversion to a gadget.
          For example, this was the case for AC/DC, until I &lt;a href="https://commons.wikimedia.org/wiki/Commons:Administrators/Requests/Lucas_Werkmeister_(interface_administrator)"&gt;became an interface administrator&lt;/a&gt;.
        &lt;/p&gt;
&lt;/details&gt;
&lt;h3 id="6"&gt;6. Gadgets should only have one maintainer&lt;/h3&gt;
&lt;h3 id="7"&gt;7. This gadget has enough maintainers&lt;/h3&gt;
&lt;h3 id="8"&gt;8. This gadget is maintained&lt;/h3&gt;
&lt;h3 id="9"&gt;9. This gadget works&lt;/h3&gt;
&lt;h3 id="10"&gt;10. If this gadget was broken, somebody would have noticed it&lt;/h3&gt;
&lt;h3 id="11"&gt;11. This gadget has users&lt;/h3&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;h3 id="12"&gt;12. This gadget has active users&lt;/h3&gt;
&lt;/summary&gt;
&lt;p&gt;
          Special:GadgetUsage
          (e.g. &lt;a href="https://en.wikipedia.org/wiki/Special:GadgetUsage"&gt;English Wikipedia&lt;/a&gt;,
          &lt;a href="https://commons.wikimedia.org/wiki/Special:GadgetUsage"&gt;Wikimedia Commons&lt;/a&gt;,
          &lt;a href="https://www.wikidata.org/wiki/Special:GadgetUsage"&gt;Wikidata&lt;/a&gt;)
          shows the number of users and active users (ones who’ve made an edit in the past 30 days) who have a gadget enabled
          (which doesn’t necessarily mean they’re actively using the gadget).
          Some gadgets can have thousands of users;
          others, even on large wikis, may only have a handful, and it’s possible that few of those users are still active.
          On test wikis you can easily find gadgets with no active users at all
          (though I’ve yet to find a gadget without even inactive users).
        &lt;/p&gt;
&lt;/details&gt;
&lt;h3 id="13"&gt;13. This gadget is used more widely than this similar user script&lt;/h3&gt;
&lt;h3 id="14"&gt;14. This gadget has no bugs&lt;/h3&gt;
&lt;h3 id="15"&gt;15. This gadget has no security vulnerabilities&lt;/h3&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;h3 id="16"&gt;16.Surely no gadget on Wikimedia wikis uses &lt;code&gt;eval()&lt;/code&gt;&lt;/h3&gt;
&lt;/summary&gt;
&lt;p&gt;
          Don’t look at &lt;a href="https://global-search.toolforge.org/?q=eval&amp;amp;namespaces=8&amp;amp;title=Gadget-.*%5C.js"&gt;this global search&lt;/a&gt; if you want to keep believing that.
        &lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;h3 id="17"&gt;17. This MediaWiki:Gadget-* page is part of a gadget&lt;/h3&gt;
&lt;/summary&gt;
&lt;p&gt;
          It’s worth checking the &lt;a href="https://www.mediawiki.org/wiki/Extension:Gadgets#Definition_format"&gt;gadgets definition page&lt;/a&gt;;
          maybe the page author never finished the gadget,
          or it became obsolete and was removed from the gadget definitions again
          (&lt;a href="https://commons.wikimedia.org/wiki/Commons:Deletion_requests/MediaWiki:Gadget-site-HideFilterBar.js"&gt;example&lt;/a&gt;).
        &lt;/p&gt;
&lt;/details&gt;
&lt;h3 id="18"&gt;18. This gadget uses a consistent coding style&lt;/h3&gt;
&lt;h3 id="19"&gt;19. This gadget’s code is written this way for a good reason, it shouldn’t be changed&lt;/h3&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;h3 id="20"&gt;20. This gadget’s code is written this way for no reason, it’s fine to change it&lt;/h3&gt;
&lt;/summary&gt;
&lt;p&gt;
          Both of these refer to &lt;a href="https://en.wikipedia.org/wiki/Chesterton's_fence"&gt;Chesterton’s fence&lt;/a&gt;,
          the idea that you shouldn’t change an existing system without understanding the reason for its current state.
          If you’re looking at an unfamiliar gadget and something in the code seems pointless and confusing,
          it’s a good idea to look into why it was written like that before changing it.
          Once you understand it, it may still be legitimate to change the code,
          e.g. if the code is working around quirks in an ancient browser version that’s no longer supported,
          or if the original developer just liked the code this way and didn’t realize that it would cause some specific bug.
          If you can’t figure out the reason,
          you’ll have to apply your own judgment whether you still want to risk changing the gadget.
        &lt;/p&gt;
&lt;/details&gt;
&lt;h3 id="21"&gt;21. Edits suggested or made by this gadget, by necessity, represent currently accepted best practices on this wiki&lt;/h3&gt;
&lt;h3 id="22"&gt;22. If this gadget was discouraged, deprecated, unmaintained, or abandoned in some way, the gadget description and/or documentation would say so&lt;/h3&gt;
&lt;h3 id="23"&gt;23. This gadget’s code is only used on this wiki&lt;/h3&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;h3 id="24"&gt;24. This gadget’s code is only used on Wikimedia wikis&lt;/h3&gt;
&lt;/summary&gt;
&lt;p&gt;
          For example, HotCat can be (and is) &lt;a href="https://commons.wikimedia.org/wiki/Help:Gadget-HotCat#Installing_HotCat_on_another_wiki"&gt;installed on many other wikis&lt;/a&gt;.
          Many of them are set up to always load the latest code straight from Wikimedia Commons,
          so we need to be extra careful when making any changes there.
        &lt;/p&gt;
&lt;/details&gt;
&lt;h3 id="25"&gt;25. This gadget is usable&lt;/h3&gt;
&lt;h3 id="26"&gt;26. This gadget’s user interface is well designed&lt;/h3&gt;
&lt;h3 id="27"&gt;27. This gadget is self-explanatory&lt;/h3&gt;
&lt;h3 id="28"&gt;28. This gadget has a documentation page&lt;/h3&gt;
&lt;h3 id="29"&gt;29. This gadget’s documentation page is up to date&lt;/h3&gt;
&lt;h3 id="30"&gt;30. This gadget’s documentation page is correct&lt;/h3&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;h3 id="31"&gt;31. Every part of this gadget is protected against vandalism&lt;/h3&gt;
&lt;/summary&gt;
&lt;p&gt;
          Translations may be less protected than the gadget’s code proper, for instance.
        &lt;/p&gt;
&lt;/details&gt;
&lt;h3 id="32"&gt;32. Any user who edited this gadget, by necessity, understood how the whole gadget worked then and still understands it now&lt;/h3&gt;
&lt;h3 id="33"&gt;33. If an issue with this gadget is reported on its talk page, somebody will respond to it&lt;/h3&gt;
&lt;h3 id="34"&gt;34. If an issue with this gadget is reported on its talk page, nobody will respond to it&lt;/h3&gt;
&lt;h3 id="35"&gt;35. An edit request to update this gadget will be processed speedily&lt;/h3&gt;
&lt;h3 id="36"&gt;36. An edit request to update this gadget will never be processed&lt;/h3&gt;
&lt;h3 id="37"&gt;37. This wiki has an edit request template&lt;/h3&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;h3 id="38"&gt;38. This wiki has a technical village pump or interface administrators’ noticeboard&lt;/h3&gt;
&lt;/summary&gt;
&lt;p&gt;
          From Wikimedia Commons, I’m mostly familiar with the &lt;a href="https://commons.wikimedia.org/wiki/Template:Edit_request"&gt;edit request&lt;/a&gt; workflow
          (a template you put on the talk page of the page you want edited),
          but e.g. German Wikipedia has no such template and instead collects all edit requests on a &lt;a href="https://de.wikipedia.org/wiki/Wikipedia:Technik/Skin/MediaWiki/%C3%84nderungen"&gt;central noticeboard&lt;/a&gt;.
          Other wikis may have neither
          (&lt;a href="https://query.wikidata.org/embed.html#SELECT%20%3Fwiki%20WHERE%20%7B%0A%20%20hint%3AQuery%20hint%3Aoptimizer%20%22None%22.%0A%20%20%3Fwiki%20wikibase%3AwikiGroup%20%22wikipedia%22.%0A%20%20MINUS%20%7B%0A%20%20%20%20%3FtemplateEditFullyProtected%20schema%3Aabout%20wd%3AQ6578653%3B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20schema%3AisPartOf%20%3Fwiki.%0A%20%20%7D%0A%20%20MINUS%20%7B%0A%20%20%20%20%3FtemplateEditProtected%20schema%3Aabout%20wd%3AQ131313714%3B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20schema%3AisPartOf%20%3Fwiki.%0A%20%20%7D%0A%20%20MINUS%20%7B%0A%20%20%20%20%3FinterfaceAdminNoticeboard%20schema%3Aabout%20wd%3AQ14720036%3B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20schema%3AisPartOf%20%3Fwiki.%0A%20%20%7D%0A%20%20MINUS%20%7B%0A%20%20%20%20%3FtechnicalVillagePump%20schema%3Aabout%20wd%3AQ4582194%3B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20schema%3AisPartOf%20%3Fwiki.%0A%20%20%7D%0A%7D"&gt;Wikidata query&lt;/a&gt;).
        &lt;/p&gt;
&lt;/details&gt;
&lt;h3 id="39"&gt;39. This gadget’s messages are translatable&lt;/h3&gt;
&lt;h3 id="40"&gt;40. Everyone can update this gadget’s translations&lt;/h3&gt;
&lt;h3 id="41"&gt;41. Everyone knows how to update this gadget’s translations&lt;/h3&gt;
&lt;h3 id="42"&gt;42. Someone knows how to update this gadget’s translations&lt;/h3&gt;
&lt;h3 id="43"&gt;43. This gadget’s source code and comments are written in English&lt;/h3&gt;
&lt;h3 id="44"&gt;44. This gadget’s source code is commented&lt;/h3&gt;
&lt;h3 id="45"&gt;45. This user script should be a gadget&lt;/h3&gt;
&lt;h3 id="46"&gt;46. This user script shouldn’t be a gadget&lt;/h3&gt;
&lt;h3 id="47"&gt;47. This user script could’ve been made a gadget already&lt;/h3&gt;
&lt;h3 id="48"&gt;48. There’s a good reason why this user script wasn’t already made a gadget&lt;/h3&gt;
&lt;h3 id="49"&gt;49. When it’s agreed that this user script should be made a gadget, it’s clear which user’s version of the user script should be adopted&lt;/h3&gt;
&lt;h3 id="50"&gt;50. This useful functionality already exists as a gadget&lt;/h3&gt;
&lt;h3 id="51"&gt;51. There must be a reason why there’s no gadget for this useful functionality yet&lt;/h3&gt;
&lt;h3 id="52"&gt;52. This gadget would be really easy to create&lt;/h3&gt;
&lt;details open=""&gt;
&lt;summary&gt;
&lt;h3 id="53"&gt;53. This gadget would be really hard to create&lt;/h3&gt;
&lt;/summary&gt;
&lt;p&gt;
          You won’t know until you try it.
          If you know how to write JavaScript,
          and you want a certain gadget or user script to exist,
          go ahead and try it out :)
        &lt;/p&gt;
&lt;/details&gt;

&lt;/article&gt;</description><guid isPermaLink="true">https://lucaswerkmeister.de/posts/2025/08/12/falsehoods-wikimedians-believe-about-gadgets/</guid><pubDate>Tue, 12 Aug 2025 00:00:00 GMT</pubDate></item><item><title>Migrating my tools to the Toolforge Build Service</title><link>https://lucaswerkmeister.de/posts/2025/07/29/migrating-tools-to-buildservice/</link><description>&lt;article&gt;

&lt;p&gt;
        Over the past few weeks, I migrated almost all of my &lt;a href="https://meta.wikimedia.org/wiki/User:Lucas_Werkmeister#Tools"&gt;tools&lt;/a&gt;
        to the &lt;a href="https://wikitech.wikimedia.org/wiki/Help:Toolforge/Building_container_images"&gt;Toolforge Build Service&lt;/a&gt;,
        and I thought it would be useful to write a bit about the process and my motivation for doing it.
      &lt;/p&gt;
&lt;h2&gt;Why I did it&lt;/h2&gt;
&lt;p&gt;
        Recently, the Wikimedia Cloud Services team announced the
        &lt;a href="https://lists.wikimedia.org/hyperkitty/list/cloud-announce@lists.wikimedia.org/thread/5D7NK7Z7KMWQPWQC23453YB7FV555Q5R/"&gt;Toolforge push-to-deploy beta&lt;/a&gt;,
        which makes it possible to set up integration with a code forge such as Wikimedia Gitlab
        that will cause newly pushed versions of a tool’s code to be deployed to Toolforge automatically.
        This has the potential to significantly simplify the development of tools:
        instead of having to log into a Toolforge bastion server and deploy every update to the tool manually,
        one can just run &lt;code&gt;git push&lt;/code&gt; and everything else happens automatically.
      &lt;/p&gt;
&lt;p&gt;
        Currently, the beta has some limitations:
        most importantly, web services are not supported yet,
        which means the feature is actually useless to me in its current state because all of my tools are web services.
        (It should already useful for bots, though it’s not clear to me if any bots already use it in practice;
        at least I couldn’t find any relevant-looking push-to-deploy config on &lt;a href="https://codesearch.wmcloud.org/"&gt;MediaWiki Codesearch&lt;/a&gt;.)
        However, I’m hopeful that support for web service will be added soon.
        In the meantime, because it already seems clear that this support will only include tools based on the build service
        (but not tools using the various other web service types supported by Toolforge),
        now seems like a good time to migrate my tools to the build service,
        so that I’ll have less work to do to set up push-to-deploy once it becomes available.
      &lt;/p&gt;
&lt;h2&gt;What I did&lt;/h2&gt;
&lt;p&gt;
        I also used this as an opportunity to adopt some best practices in my tools in general,
        even if not all of them were related to the build service migration.
        I’ll go through them here in roughly the order in which I did them in most tools.
      &lt;/p&gt;
&lt;h3&gt;Add health check&lt;/h3&gt;
&lt;p&gt;
        A &lt;a href="https://wikitech.wikimedia.org/wiki/Help:Toolforge/Web#Health_checks"&gt;health check&lt;/a&gt;
        is a way for the Toolforge infrastructure to detect if a tool is running (“healthy”) or not.
        This is useful, for instance, to enable restarts of a tool (including deploying new versions) with no downtime:
        the infrastructure (Kubernetes) will bring up a container with the new version of the tool,
        wait for it to become ready according to the health check,
        switch traffic from the old container to the new one,
        and only then tear down the old container.
      &lt;/p&gt;
&lt;p&gt;
        Since 2024, Toolforge installs a TCP health check by default:
        the tool is considered healthy if it accepts connections on the web service port.
        However, this doesn’t guarantee that the server is actually ready to handle requests;
        we can do better by defining a &lt;code&gt;health-check-path&lt;/code&gt; in the
        &lt;a href="https://wikitech.wikimedia.org/wiki/Help:Toolforge/Web#Webservice_templates"&gt;&lt;code&gt;service.template&lt;/code&gt; file&lt;/a&gt;,
        at which point Toolforge will instead use an HTTP health check and test if the tool successfully responds to HTTP requests to this path.
        It’s apparently conventional to call this path &lt;code&gt;/healthz&lt;/code&gt;
        (though last I looked, nobody seemed to know what the “z” stands for),
        and as it doesn’t need to return anything special,
        the Python code for this endpoint looks very simple:
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class="entity name function decorator"&gt;@app.route&lt;/span&gt;(&lt;span class="string"&gt;'/healthz'&lt;/span&gt;)
&lt;span class="storage function"&gt;def&lt;/span&gt; &lt;span class="entity name function"&gt;health&lt;/span&gt;():
    &lt;span class="keyword"&gt;return&lt;/span&gt; &lt;span class="string"&gt;''&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        (Plus a &lt;a href="https://github.com/pallets/flask/blob/3.1.1/src/flask/typing.py#L37"&gt;return type&lt;/a&gt;,
        import-aliased to &lt;abbr title="ResponseReturnValue"&gt;RRV&lt;/abbr&gt;,
        in those tools where I use mypy.)
        And it’s configured in the &lt;code&gt;service.template&lt;/code&gt; file like this:
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;health-check-path: /healthz&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        I usually did this improvement first (unless I forgot or it was already set up)
        because it meant that most of the following improvements could be deployed without downtime for users.
      &lt;/p&gt;
&lt;h3&gt;Splitting prod and dev dependencies&lt;/h3&gt;
&lt;p&gt;
        In most of my tools, I previously had only one &lt;code&gt;requirements.txt&lt;/code&gt; file
        (compiled using &lt;a href="https://pip-tools.readthedocs.io/en/latest/"&gt;pip-tools&lt;/a&gt; from &lt;code&gt;requirements.in&lt;/code&gt;).
        This means that the tool’s installation on Toolforge included
        not just the packages required to run the tool (Flask, Werkzeug, mwapi, etc.)
        but also the packages required to test it (Flake8, mypy, pytest, etc.).
        This is wasteful (mypy is big!),
        and a build service based tool would install its dependencies more often than before
        (each time a new image is built, i.e. during every deployment),
        so I took an improvement I’d already done years ago in the Wikidata Lexeme Forms tool
        and followed through with it in my other tools:
        split the testing packages into a separate file
        (&lt;code&gt;dev-requirements.txt&lt;/code&gt;, compiled from &lt;code&gt;dev-requirements.in&lt;/code&gt;).
        The dev packages are installed locally (&lt;code&gt;pip-sync *requirements.txt&lt;/code&gt;)
        and in CI (&lt;code&gt;pip install -r requirements.txt -r dev-requirements.txt&lt;/code&gt;),
        but not on Toolforge.
        In most tools, this shrunk the installed venv roughly by 50%, which is pretty neat!
      &lt;/p&gt;
&lt;p&gt;
        I also added a CI job that verifies that I didn’t accidentally put a prod dependency into the dev requirements,
        by only installing the prod requirements and checking that &lt;code&gt;python app.py&lt;/code&gt; runs through without crashing on a missing import.
        This isn’t perfect,
        but since I know that I’m not doing any advanced lazy-import stuff in my own code,
        it’s good enough for me.
        (I guess an alternative would be to reuse the health check for this.)
      &lt;/p&gt;
&lt;h3&gt;Configuration from environment variables&lt;/h3&gt;
&lt;p&gt;
        All of my Flask tool read the &lt;a href="https://flask.palletsprojects.com/en/stable/config/"&gt;Flask configuration&lt;/a&gt;
        from a (user-only-readable) &lt;code&gt;config.yaml&lt;/code&gt; file in the source code directory;
        this contains, at a minimum, the &lt;a href="https://flask.palletsprojects.com/en/stable/quickstart/#sessions"&gt;secret key&lt;/a&gt; used to sign the session,
        and sometimes more information, such as the OAuth consumer key and secret.
        This is still possible on the build service (by specifying the &lt;code&gt;mount: all&lt;/code&gt; option),
        but it means the tool will rely on NFS, which is generally undesirable.
        A more forward-looking option is to store the config in &lt;a href="https://wikitech.wikimedia.org/wiki/Help:Toolforge/Envvars"&gt;environment variables&lt;/a&gt;,
        which Toolforge added support for two years ago.
      &lt;/p&gt;
&lt;p&gt;
        It turns out that Flask has a method, &lt;a href="https://flask.palletsprojects.com/en/stable/api/#flask.Config.from_prefixed_env"&gt;&lt;code&gt;app.config.from_prefixed_env()&lt;/code&gt;&lt;/a&gt;,
        which will automatically load all environment variables whose name starts with a certain prefix (I use &lt;code&gt;TOOL_&lt;/code&gt;) into the config.
        It even has support for nested objects (using double underscores in the name),
        so that configuration like &lt;code&gt;app.config['OAUTH']['consumer_key']&lt;/code&gt;
        can be represented as the environment variable named &lt;code&gt;TOOL_OAUTH__consumer_key&lt;/code&gt;.
      &lt;/p&gt;
&lt;p&gt;
        However, there’s one problem with this:
        Toolforge &lt;a href="https://phabricator.wikimedia.org/T374780"&gt;requires environment variables to have all-uppercase names&lt;/a&gt;,
        but my existing code was expecting lowercase names inside the &lt;code&gt;OAUTH&lt;/code&gt; config dict.
        I worked around this by first converting the configuration keys to all-uppercase
        (initially, still inside the &lt;code&gt;config.yaml&lt;/code&gt; file);
        then, I moved the configuration to envvars,
        and finally commented out the contents of the &lt;code&gt;config.yaml&lt;/code&gt; file
        (&lt;a href="https://sal.toolforge.org/tools.speedpatrolling?d=2025-07-16"&gt;example &lt;abbr title="Server Admin Log"&gt;SAL&lt;/abbr&gt;&lt;/a&gt;).
        All of this was possible while the tools were still running on the legacy web service types.
        (The code reading the &lt;code&gt;config.yaml&lt;/code&gt; file is still there, by the way –
        it’s much more convenient for local development, even if it’s not used on Toolforge anymore.)
      &lt;/p&gt;
&lt;h3&gt;Move CI from GitHub to GitLab&lt;/h3&gt;
&lt;p&gt;
        The CI for most of my tools was on GitHub,
        mainly because many of them predated Wikimedia GitLab (or the availability of GitLab CI there).
        However, I don’t really fancy giving Microsoft deploy access to my tools,
        so I moved the CI over to GitLab CI.
        For most tools, this was very straightforward,
        to the point where I just copy+pasted the &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; file between tools.
        (In QuickCategories, setting up a MariaDB service for CI required a little bit more work.)
      &lt;/p&gt;
&lt;h3&gt;Actual build service migration&lt;/h3&gt;
&lt;p&gt;
        The migration to the build service starts with the &lt;a href="https://wikitech.wikimedia.org/wiki/Help:Toolforge/Building_container_images#Procfile"&gt;&lt;code&gt;Procfile&lt;/code&gt;&lt;/a&gt;,
        which tells the infrastructure how to run the tool.
        I used the same &lt;code&gt;Procfile&lt;/code&gt; for all my Python tools:
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;web: gunicorn --workers=4 app:app&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        This defines an entrypoint called &lt;code&gt;web&lt;/code&gt;
        which will run &lt;a href="https://gunicorn.org/"&gt;Gunicorn&lt;/a&gt;,
        with four worker processes, importing &lt;code&gt;app.py&lt;/code&gt; and running the &lt;code&gt;app&lt;/code&gt; WSGI app from it.
        Toolforge specifies the &lt;code&gt;$PORT&lt;/code&gt; environment variable to tell the tool where to listen for connections,
        and Gunicorn will &lt;a href="https://docs.gunicorn.org/en/stable/settings.html#bind"&gt;bind to that port by default&lt;/a&gt; if the environment variable is defined,
        so no explicit &lt;code&gt;--bind&lt;/code&gt; option is necessary.
        Of course, this also requires adding &lt;code&gt;gunicorn&lt;/code&gt; to &lt;code&gt;requirements.in&lt;/code&gt; / &lt;code&gt;requirements.txt&lt;/code&gt;,
        so that it will be installed inside the image.
        Also, don’t forget to &lt;code&gt;git add Procfile&lt;/code&gt;…
      &lt;/p&gt;
&lt;p&gt;
        A significant benefit of the build service is that it gives us early access to newer Python versions.
        By writing &lt;code&gt;3.13&lt;/code&gt; in a file called &lt;code&gt;.python-version&lt;/code&gt;
        (don’t forget to &lt;code&gt;git add&lt;/code&gt; this one either!),
        and specifying &lt;code&gt;--use-latest-versions&lt;/code&gt; when running &lt;code&gt;toolforge build start&lt;/code&gt;
        (presumably this will become the default at some point),
        our tool will run on Python 3.13,
        whereas the latest version available outside of the build service is currently Python 3.11
        (&lt;a href="https://phabricator.wikimedia.org/T381899#11027628"&gt;until two weeks or so from now&lt;/a&gt;).
        I didn’t actually notice any Python 3.13 features I wanted to use in my tools
        (except for one tool where I was able to replace a &lt;code&gt;TypeAlias&lt;/code&gt; annotation with a &lt;code&gt;type&lt;/code&gt; statement),
        but it’s still nice to use the same version in production as the one I develop on locally.
        (Of course, I also bumped the Python version in CI from 3.11 to 3.13.)
      &lt;/p&gt;
&lt;p&gt;
        That said, there is one issue with Python 3.13 that I had to work around.
        All of my Python tools use the &lt;a href="https://python-toolforge.readthedocs.io/en/latest/"&gt;&lt;code&gt;toolforge&lt;/code&gt; library&lt;/a&gt;
        for its &lt;a href="https://python-toolforge.readthedocs.io/en/latest/usage.html#set-policy-compliant-user-agent"&gt;&lt;code&gt;set_user_agent()&lt;/code&gt; function&lt;/a&gt;
        (it has other features but I mostly don’t use them);
        this library imports &lt;a href="https://pypi.org/project/PyMySQL/"&gt;PyMySQL&lt;/a&gt; as soon as it is imported.
        PyMySQL, in turn, immediately tries to initialize a default user name for database connections from the environment
        (even if the tool is never going to open a database connection),
        via the Python &lt;code&gt;getpass.getuser()&lt;/code&gt; function.
        However, inside a build service container, no user name is set, and so this function raises an error.
        This was fine in earlier Python versions, because PyMySQL catches the error;
        however, Python 3.13 changed the error being thrown from &lt;code&gt;KeyError&lt;/code&gt; to &lt;code&gt;OSError&lt;/code&gt;,
        which PyMySQL didn’t catch.
        PyMySQL subsequently &lt;a href="https://github.com/PyMySQL/PyMySQL/commit/a1ac8239c8"&gt;added this error to the &lt;code&gt;except&lt;/code&gt; clause&lt;/a&gt;;
        however, they haven’t published a new release since that commit.
        Due to this bizarre confluence of edge cases,
        it’s impossible to import &lt;code&gt;toolforge&lt;/code&gt; or &lt;code&gt;pymysql&lt;/code&gt; in a Toolforge Build Service tool on Python 3.13 or later when using the latest released version of PyMySQL.
        My workaround is to install PyMySQL from Git, using this &lt;code&gt;requirements.in&lt;/code&gt; entry:
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pymysql @ git+https://github.com/PyMySQL/PyMySQL@main&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        I look forward to the day when I’ll be able to remove this again.
      &lt;/p&gt;
&lt;p&gt;
        The remaining part of the build service migration is the &lt;code&gt;service.template&lt;/code&gt; file,
        which contains default arguments for calling &lt;code&gt;webservice&lt;/code&gt; commands.
        I changed the &lt;code&gt;type&lt;/code&gt; from &lt;code&gt;python3.11&lt;/code&gt; to &lt;code&gt;buildservice&lt;/code&gt;,
        and also added &lt;code&gt;mount: none&lt;/code&gt; to specify that the tool doesn’t need NFS mounted.
        Then, after pushing the changes to GitLab and building a new container image,
        I deployed the build service version with commands like this:
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;webservice stop &lt;span class="keyword operator"&gt;&amp;amp;&lt;/span&gt;&lt;span class="keyword operator"&gt;&amp;amp;&lt;/span&gt;
mv www{,-unused-tool-now-runs-on-buildservice} &lt;span class="keyword operator"&gt;&amp;amp;&lt;/span&gt;&lt;span class="keyword operator"&gt;&amp;amp;&lt;/span&gt;
wget https://gitlab.wikimedia.org/toolforge-repos/translate-link/-/raw/2e2349a9fb/service.template &lt;span class="keyword operator"&gt;&amp;amp;&lt;/span&gt;&lt;span class="keyword operator"&gt;&amp;amp;&lt;/span&gt;
webservice start&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        This stops the webservice (using the old defaults in &lt;code&gt;www/python/src/service.template&lt;/code&gt;),
        moves the old source code directory away so I don’t get confused by it later (I’ll remove it eventually™),
        downloads the new &lt;code&gt;service.template&lt;/code&gt; file right into the home directory,
        and then starts the webservice again using the defaults from that file.
        And last but not least, I updated the instructions in the &lt;code&gt;README.md&lt;/code&gt;
        (initially as a separate commit,
        later in the same big migration commit because I couldn’t be bothered to separate it anymore).
      &lt;/p&gt;
&lt;h2&gt;More details&lt;/h2&gt;
&lt;p&gt;
        If you want to follow the migrations in more detail,
        you can look at the relevant Git commit ranges and &lt;abbr title="Server Admin Log"&gt;SAL&lt;/abbr&gt; entries:
      &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wikidata Lexeme Forms: &lt;a href="https://gitlab.wikimedia.org/toolforge-repos/lexeme-forms/-/compare/8c32f8b90c...3c977ccc7b"&gt;Git&lt;/a&gt;, &lt;a href="https://sal.toolforge.org/tools.lexeme-forms?d=2025-07-13"&gt;SAL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wikidata Image Positions: &lt;a href="https://gitlab.wikimedia.org/toolforge-repos/wd-image-positions/-/compare/b2bc6237f0...256e149618"&gt;Git&lt;/a&gt;, &lt;a href="https://sal.toolforge.org/tools.wd-image-positions?d=2025-07-15"&gt;SAL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;QuickCategories (was mostly already migrated to the build service from &lt;a href="https://phabricator.wikimedia.org/T374152"&gt;T374152&lt;/a&gt;): &lt;a href="https://gitlab.wikimedia.org/toolforge-repos/quickcategories/-/compare/1be5ca731c...75b652fb6e"&gt;Git&lt;/a&gt;, &lt;a href="https://sal.toolforge.org/tools.quickcategories?d=2025-07-15"&gt;SAL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SpeedPatrolling: &lt;a href="https://gitlab.wikimedia.org/toolforge-repos/speedpatrolling/-/compare/9d8920826c...5c34e3284f"&gt;Git&lt;/a&gt;, &lt;a href="https://sal.toolforge.org/tools.speedpatrolling?d=2025-07-26"&gt;SAL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PagePile Visual Filter: &lt;a href="https://gitlab.wikimedia.org/toolforge-repos/pagepile-visual-filter/-/compare/975e4e6800...ed716e5378"&gt;Git&lt;/a&gt;, &lt;a href="https://sal.toolforge.org/tools.pagepile-visual-filter?d=2025-07-26"&gt;SAL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ranker: &lt;a href="https://gitlab.wikimedia.org/toolforge-repos/ranker/-/compare/e059817c66...60cc18c07b"&gt;Git&lt;/a&gt;, &lt;a href="https://sal.toolforge.org/tools.ranker?d=2025-07-23"&gt;SAL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Translate Link: &lt;a href="https://gitlab.wikimedia.org/toolforge-repos/translate-link/-/compare/02b7a10a31...e1efca1601"&gt;Git&lt;/a&gt;, &lt;a href="https://sal.toolforge.org/tools.translate-link?d=2025-07-25"&gt;SAL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
        At some point, I should also apply most of these improvements to &lt;a href="https://github.com/lucaswerkmeister/cookiecutter-toolforge"&gt;cookiecutter-toolforge&lt;/a&gt;,
        though I’m not so sure about the split-requirements part
        (I feel like it might overcomplicate the dev setup for other developers for little benefit).
        Let me know what you think :)
      &lt;/p&gt;

&lt;/article&gt;</description><guid isPermaLink="true">https://lucaswerkmeister.de/posts/2025/07/29/migrating-tools-to-buildservice/</guid><pubDate>Tue, 29 Jul 2025 00:00:00 GMT</pubDate></item><item><title>Wikimedia Hackathon 2025 recap</title><link>https://lucaswerkmeister.de/posts/2025/05/10/wikimedia-hackathon-2025/</link><description>&lt;article&gt;

&lt;p&gt;
        A week ago, I took part in the &lt;a href="https://www.mediawiki.org/wiki/Wikimedia_Hackathon_2025"&gt;Wikimedia Hackathon 2025&lt;/a&gt;,
        which took place on 2–4 May (Friday–Sunday) in Istanbul, Turkey.
        Just like &lt;a href="https://lucaswerkmeister.de/posts/2024/05/15/wikimedia-hackathon-2024/"&gt;last year&lt;/a&gt;
        and &lt;a href="https://lucaswerkmeister.de/posts/2023/06/03/wikimedia-hackathon-2023/"&gt;the year before&lt;/a&gt;,
        I want to write a bit about the experience.
      &lt;/p&gt;
&lt;p&gt;
        I had come into the hackathon with two vague project ideas:
        work on the migration of m3api to Wikimedia GitLab (&lt;a href="https://phabricator.wikimedia.org/T392290"&gt;T392290&lt;/a&gt;),
        especially the documentation (&lt;a href="https://phabricator.wikimedia.org/T392716"&gt;T392716&lt;/a&gt;),
        and continue making local language names translatable on translatewiki.net (&lt;a href="https://phabricator.wikimedia.org/T231755"&gt;T231755&lt;/a&gt;).
        But as sometimes happens at such events, things turned out otherwise.
      &lt;/p&gt;
&lt;p&gt;
        During the travel to the hackathon (i.e. at the airport) and on Thursday evening,
        I got a decent amount of work on m3api in:
        I mostly managed to port the documentation-building release CI to GitLab actions
        (though I’ll still need to get access to push it to doc.wikimedia.org).
        However, also during that evening, in a dinner conversatiaon with &lt;a href="https://www.mediawiki.org/wiki/User:HNordeen_(WMF)"&gt;Haley Nordeen&lt;/a&gt;,
        we came across the idea of “Redactle for Wikidata” on the venerable
        &lt;a href="https://phabricator.wikimedia.org/T165167"&gt;building more games using Wikidata’s data&lt;/a&gt; task.
        During the &lt;a href="https://phabricator.wikimedia.org/T392540"&gt;game ideas session&lt;/a&gt; the next morning,
        I decided to try this out, and it ended up becoming my main project of the hackathon.
        (I didn’t end up working on the local language names at all in the end.)
      &lt;/p&gt;
&lt;p&gt;
        What I had by the time of the showcase on Sunday was not a finished product,
        but at least a playable version of the game,
        called &lt;a href="https://wdactle.toolforge.org/"&gt;WDactle&lt;/a&gt; (&lt;a href="https://gitlab.wikimedia.org/toolforge-repos/wdactle/"&gt;source code&lt;/a&gt;).
        There’s no “puzzle of the day” yet (like in Wordle or Redactle),
        just a random puzzle each time you load the page (cached for five minutes).
        And despite the missing features, the game seems to be feasible in principle,
        and more fun than I expected,
        both according to my own experience and what I’m hearing from others 🙂
        so I’ll definitely continue working on it.
        (Special thanks to &lt;a href="https://meta.wikimedia.org/wiki/User:SSanchez-WMF"&gt;Sarai Sánchez&lt;/a&gt;
        for talking through the design with me on Saturday evening!)
      &lt;/p&gt;
&lt;p&gt;
        Of course, the hackathon isn’t just about hacking on your own projects.
        I don’t think I directly worked on anyone else’s project,
        but I was at least able to give some useful pointers and advice to several people.
        I had also announced in the opening session that I could hand out some invite codes to &lt;a href="https://meta.wikimedia.org/wiki/Wikis_World"&gt;Wikis World&lt;/a&gt;,
        and I’m happy to report that one invite code was successfully exchanged and used!
        And I joined some sessions on the program, including several related to Wikimedia Toolforge and tool development,
        and one on the future of the MediaWiki Action API.
      &lt;/p&gt;
&lt;p&gt;
        On the more social side, I talked to or hung out with a fair amount of people,
        ranging from an impromptu meeting of the &lt;a href="https://wikitech.wikimedia.org/wiki/Help:Toolforge/Toolforge_standards_committee"&gt;Toolforge Standards Committee&lt;/a&gt;
        to a spontaneous magic show (yes!).
        The “juggling + rubik’s cubes” session from the last two years didn’t really happen again,
        but we still had some social time before the Kahoot session on Saturday afternoon.
        I also restocked the sweets table with chocolate several times.
        Sadly, although Taavi and I brought our blåhajar,
        we didn’t have a proper Wikimedia Cuteness Association meetup this year 😔
      &lt;/p&gt;
&lt;p&gt;
        As usual, I posted about my hackathon experience on Mastodon,
        this time with separate threads for &lt;a href="https://wikis.world/@LucasWerkmeister/114430682133543886"&gt;Thursday&lt;/a&gt;,
        &lt;a href="https://wikis.world/@LucasWerkmeister/114436950287170418"&gt;Friday&lt;/a&gt;,
        &lt;a href="https://wikis.world/@LucasWerkmeister/114442409903718033"&gt;Saturday&lt;/a&gt;,
        &lt;a href="https://wikis.world/@LucasWerkmeister/114447994481484488"&gt;Sunday&lt;/a&gt; and
        &lt;a href="https://wikis.world/@LucasWerkmeister/114453519361216347"&gt;Monday&lt;/a&gt;.
        As you can see there, I traveled to and from the hackathon by plane this year;
        I looked into other options (ever heard of this thing called the “orient express”??),
        but didn’t think that any of them looked feasible to me.
        (Clearly I should’ve coordinated with Pintoch, who apparently found a way after all!)
        I hope next year will be a little bit closer to Berlin again 🙂
      &lt;/p&gt;

&lt;/article&gt;</description><guid isPermaLink="true">https://lucaswerkmeister.de/posts/2025/05/10/wikimedia-hackathon-2025/</guid><pubDate>Sat, 10 May 2025 00:00:00 GMT</pubDate></item><item><title>Introducing m3api</title><link>https://lucaswerkmeister.de/posts/2025/04/12/introducing-m3api/</link><description>&lt;article&gt;

&lt;p&gt;
        For the past couple of years, I’ve been working on a new JavaScript library for the MediaWiki Action API, called &lt;strong&gt;m3api&lt;/strong&gt;.
        On the occasion of its 1.0.0 release today,
        I want to talk about why I wrote it, what it does, and why I think you should use it :)
      &lt;/p&gt;
&lt;h2 id="quick-links"&gt;Quick links&lt;/h2&gt;
&lt;p&gt;
&lt;a href="https://www.npmjs.com/package/m3api"&gt;npm package&lt;/a&gt;,
        &lt;a href="https://github.com/lucaswerkmeister/m3api/"&gt;GitHub repository&lt;/a&gt;,
        &lt;a href="https://lucaswerkmeister.github.io/m3api/"&gt;documentation&lt;/a&gt;,
        &lt;a href="https://github.com/lucaswerkmeister/m3api-examples/"&gt;examples&lt;/a&gt;.
      &lt;/p&gt;
&lt;h2 id="why-a-new-library"&gt;Why a new JS library for the MediaWiki API?&lt;/h2&gt;
&lt;p&gt;
        So why did I write a new library for the MediaWiki API at all?
        Aren’t there &lt;a href="https://www.mediawiki.org/wiki/API:Client_code/All#JavaScript"&gt;enough of them&lt;/a&gt; already?
      &lt;/p&gt;
&lt;p&gt;
        I was looking for a library fulfilling two criteria,
        and didn’t find any that fulfilled both:
      &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
          Cross-platform: I want to be able to use the same interface to the API whether I’m writing code for the browser or for Node.js.
          (Small differences in setup are acceptable, but once setup is done, the interface should be uniform.)
          This apparently rules out virtually all the libraries;
          the only known exception on the list of libraries linked above (apart from m3api itself)
          is &lt;a href="https://github.com/kanasimi/CeJS"&gt;CeJS&lt;/a&gt;, which is a mystery to me.
          &lt;!-- This phrasing is kind of ambiguous: it could mean “CeJS is a mystery” or “the fact that no other library is cross-platform is a mystery”. But I agree with both ^^ --&gt;
&lt;/li&gt;
&lt;li&gt;
          Reasonably modern: at a minimum, this means promises rather than callbacks.
          (As far as I can tell, this rules out CeJS, along with many other libraries.)
          Additional modern things that would be nice to have
          are &lt;a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/AsyncGenerator"&gt;async generators&lt;/a&gt; as the interface for API continuation
          and &lt;a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules"&gt;ES6 modules&lt;/a&gt; instead of Node.js &lt;code&gt;require()&lt;/code&gt; / UMD / etc.
        &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;
        Since I couldn’t find a library matching my needs, I wrote it :)
      &lt;/p&gt;
&lt;h2 id="main-characteristics"&gt;Main characteristics&lt;/h2&gt;
&lt;p&gt;
&lt;a href="https://www.martinfowler.com/bliki/TwoHardThings.html"&gt;Naming things is hard&lt;/a&gt;;
        m3api stands for “&lt;strong&gt;minimal, modern MediaWiki API [client]&lt;/strong&gt;” (three ‘m’s, you see).
        I’ve already mentioned “modern” above –
        m3api uses promises, async generators, ES6 modules,
        but also &lt;code&gt;fetch()&lt;/code&gt; (even in Node – yay for &lt;a href="https://nodejs.org/en/learn/getting-started/fetch"&gt;undici&lt;/a&gt;),
        &lt;code&gt;class&lt;/code&gt; syntax, object spreading and destructuring,
        &lt;code&gt;FormData&lt;/code&gt; / &lt;code&gt;Blob&lt;/code&gt; / &lt;code&gt;File&lt;/code&gt; for file parameters, and more.
        (Some of this felt fairly “bleeding edge” when I started working on m3api,
        but keep in mind that this was almost five years ago.
        m3api may not support all the &lt;a href="https://www.mediawiki.org/wiki/Compatibility#Browsers"&gt;browsers supported by MediaWiki&lt;/a&gt;,
        but it does support the Node.js version that was shipped in stable Debian 12 (Bookworm) two years ago.)
      &lt;/p&gt;
&lt;p&gt;
        I want to elaborate on the “minimal” term a bit more.
        Basically, the point is that I’m familiar with the MediaWiki Action API,
        and I don’t like libraries that aim to hide the API from me.
        I’m wary of basic &lt;a href="https://en.wikipedia.org/wiki/Create,_read,_update_and_delete"&gt;&lt;abbr title="create, read, update, delete"&gt;CRUD&lt;/abbr&gt;&lt;/a&gt; abstraction methods;
        the &lt;code&gt;action=edit&lt;/code&gt; API has plenty of useful options,
        many of which a higher-level method probably doesn’t make available.
        I want a library that helps me to work with the API directly.
        (I don’t mind if it &lt;em&gt;also&lt;/em&gt; offers abstraction methods, but they’re not a high priority for me when writing my own library.
        Also, some other libraries seem to make it relatively hard to make direct API requests.)
      &lt;/p&gt;
&lt;p&gt;
        However, “minimal” doesn’t mean that the library doesn’t have any features.
        There are plenty of features designed to make it easier to use the API;
        my basic rule of thumb is that the feature should be useful with more than one API action.
        For example, API continuation is present in several API actions, and somewhat tedious to use “manually”,
        so m3api offers support for it.
      &lt;/p&gt;
&lt;p&gt;
        In addition to that, there are also several extension packages for m3api,
        as well as &lt;a href="https://github.com/lucaswerkmeister/m3api/#creating-extension-packages"&gt;guidelines&lt;/a&gt;
        for others to implement additional extension packages.
        These implement support for specific API modules
        (&lt;a href="https://github.com/lucaswerkmeister/m3api-query/"&gt;m3api-query&lt;/a&gt; for &lt;code&gt;action=query&lt;/code&gt;,
        &lt;a href="https://github.com/lucaswerkmeister/m3api-botpassword/"&gt;m3api-botpassword&lt;/a&gt; for &lt;code&gt;action=login&lt;/code&gt;)
        or other functionality that doesn’t belong in m3api itself
        (&lt;a href="https://github.com/lucaswerkmeister/m3api-oauth2/"&gt;m3api-oauth2&lt;/a&gt; for the OAuth 2.0 authorization flow).
        In combination, these libraries are intended to provide,
        if not a full API framework,
        then at least a powerful and flexible toolkit for working with the API.
      &lt;/p&gt;
&lt;h2 id="basic-interface"&gt;Basic interface&lt;/h2&gt;
&lt;p&gt;
        The simplest way to make an API request with m3api looks like this:
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class="keyword"&gt;import&lt;/span&gt; Session &lt;span class="keyword"&gt;from&lt;/span&gt; &lt;span class="string"&gt;'m3api/node.js'&lt;/span&gt;;
&lt;span class="storage type"&gt;const&lt;/span&gt; session &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="keyword"&gt;new&lt;/span&gt; &lt;span class="variable type"&gt;Session&lt;/span&gt;( &lt;span class="string"&gt;'en.wikipedia.org'&lt;/span&gt; );
&lt;span class="storage type"&gt;const&lt;/span&gt; response &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="keyword"&gt;await&lt;/span&gt; session.&lt;span class="function call"&gt;request&lt;/span&gt;(
	{ action: &lt;span class="string"&gt;'query'&lt;/span&gt;, meta: &lt;span class="string"&gt;'siteinfo'&lt;/span&gt; },
);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        You can also specify default parameters that should apply to every request of a session when creating it:
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class="keyword"&gt;import&lt;/span&gt; Session &lt;span class="keyword"&gt;from&lt;/span&gt; &lt;span class="string"&gt;'m3api/node.js'&lt;/span&gt;;
&lt;span class="storage type"&gt;const&lt;/span&gt; session &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="keyword"&gt;new&lt;/span&gt; &lt;span class="variable type"&gt;Session&lt;/span&gt;(
	&lt;span class="string"&gt;'en.wikipedia.org'&lt;/span&gt;,
	{ formatversion: &lt;span class="constant numeric"&gt;2&lt;/span&gt; },
);
&lt;span class="storage type"&gt;const&lt;/span&gt; response &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="keyword"&gt;await&lt;/span&gt; session.&lt;span class="function call"&gt;request&lt;/span&gt;(
	{ action: &lt;span class="string"&gt;'query'&lt;/span&gt;, meta: &lt;span class="string"&gt;'siteinfo'&lt;/span&gt; },
);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        These examples specify &lt;em&gt;parameters&lt;/em&gt; to send to the API (&lt;code&gt;action=query&lt;/code&gt;, &lt;code&gt;meta=siteinfo&lt;/code&gt;, &lt;code&gt;formatversion=2&lt;/code&gt;).
        Additionally, you can specify &lt;em&gt;options&lt;/em&gt; as another object after the parameters,
        which instead influence how m3api sends the request.
        One option that you should always set is the &lt;code&gt;userAgent&lt;/code&gt;, which controls the &lt;code&gt;User-Agent&lt;/code&gt; HTTP header
        (see the &lt;a href="https://meta.wikimedia.org/wiki/Special:MyLanguage/User-Agent_policy"&gt;User-Agent policy&lt;/a&gt;).
        Usually, you would set this option for all requests when creating the session:
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class="keyword"&gt;import&lt;/span&gt; Session &lt;span class="keyword"&gt;from&lt;/span&gt; &lt;span class="string"&gt;'m3api/node.js'&lt;/span&gt;;
&lt;span class="storage type"&gt;const&lt;/span&gt; session &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="keyword"&gt;new&lt;/span&gt; &lt;span class="variable type"&gt;Session&lt;/span&gt;(
	&lt;span class="string"&gt;'en.wikipedia.org'&lt;/span&gt;,
	{ formatversion: &lt;span class="constant numeric"&gt;2&lt;/span&gt; },
	{ userAgent: &lt;span class="string"&gt;'introducing-m3api-blog-post'&lt;/span&gt; },
);
&lt;span class="storage type"&gt;const&lt;/span&gt; response &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="keyword"&gt;await&lt;/span&gt; session.&lt;span class="function call"&gt;request&lt;/span&gt;(
	{ action: &lt;span class="string"&gt;'query'&lt;/span&gt;, meta: &lt;span class="string"&gt;'siteinfo'&lt;/span&gt; },
);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        But you could also set it on the individual request, if you wanted:
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class="keyword"&gt;import&lt;/span&gt; Session &lt;span class="keyword"&gt;from&lt;/span&gt; &lt;span class="string"&gt;'m3api/node.js'&lt;/span&gt;;
&lt;span class="storage type"&gt;const&lt;/span&gt; session &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="keyword"&gt;new&lt;/span&gt; &lt;span class="variable type"&gt;Session&lt;/span&gt;(
	&lt;span class="string"&gt;'en.wikipedia.org'&lt;/span&gt;,
	{ formatversion: &lt;span class="constant numeric"&gt;2&lt;/span&gt; },
);
&lt;span class="storage type"&gt;const&lt;/span&gt; response &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="keyword"&gt;await&lt;/span&gt; session.&lt;span class="function call"&gt;request&lt;/span&gt;(
	{ action: &lt;span class="string"&gt;'query'&lt;/span&gt;, meta: &lt;span class="string"&gt;'siteinfo'&lt;/span&gt; },
	{ userAgent: &lt;span class="string"&gt;'introducing-m3api-blog-post'&lt;/span&gt; },
);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        (It doesn’t make much sense to set the &lt;code&gt;userAgent&lt;/code&gt; per request,
        but there are other options where it’s more useful,
        e.g. &lt;code&gt;method: 'POST'&lt;/code&gt; and &lt;code&gt;tokenType: 'csrf'&lt;/code&gt;.)
      &lt;/p&gt;
&lt;p&gt;
        Other functions generally also follow this pattern of taking parameters followed by options,
        with the options being, well, optional.
        Both the parameters and options are merged with the defaults from the constructor,
        making for a convenient and uniform interface.
      &lt;/p&gt;
&lt;p&gt;
        In addition to strings, parameter values can also be numbers, booleans, and arrays, for example:
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class="storage type"&gt;const&lt;/span&gt; response &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="keyword"&gt;await&lt;/span&gt; session.&lt;span class="function call"&gt;request&lt;/span&gt;( {
	action: &lt;span class="string"&gt;'query'&lt;/span&gt;,
	meta: [ &lt;span class="string"&gt;'siteinfo'&lt;/span&gt;, &lt;span class="string"&gt;'userinfo'&lt;/span&gt; ],
	curtimestamp: &lt;span class="constant language"&gt;true&lt;/span&gt;,
	formatversion: &lt;span class="constant numeric"&gt;2&lt;/span&gt;,
} );&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        List parameters can also be sets instead of arrays; more on that &lt;a href="https://lucaswerkmeister.de/posts/2025/04/12/introducing-m3api/#combining-requests"&gt;below&lt;/a&gt;.
      &lt;/p&gt;
&lt;h2 id="api-continuation"&gt;API continuation&lt;/h2&gt;
&lt;p&gt;
        As mentioned above, m3api includes support for API continuation.
        I’m not aware of a great explanation of this feature in the API,
        so I’ll just use this section to talk about it in general as well as how m3api supports it ^^
      &lt;/p&gt;
&lt;p&gt;
&lt;em&gt;Continuation&lt;/em&gt; is the mechanism by which the API returns a limited set of data
        while enabling you to make further requests to fetch additional data.
        The MediaWiki Action API’s continuation mechanism is highly flexible;
        a single API request can use many different modules, each of which contributes to continuation,
        and it all works out.
      &lt;/p&gt;
&lt;p&gt;
        The basic principle is that the API may return,
        as part of the response,
        a &lt;code&gt;continue&lt;/code&gt; object with parameters you should send with your next request.
        For instance, if you make an API request with &lt;code&gt;action=query&lt;/code&gt; and &lt;code&gt;list=allpages&lt;/code&gt;,
        the response may include &lt;code&gt;"continue": { "apcontinue": "!important" }&lt;/code&gt;;
        your next request should then use the parameters
        &lt;code&gt;action=query&lt;/code&gt;, &lt;code&gt;list=allpages&lt;/code&gt; and &lt;code&gt;apcontinue=!important&lt;/code&gt;.
        Continuation is finished when there is no &lt;code&gt;continue&lt;/code&gt; object in a response.
      &lt;/p&gt;
&lt;p&gt;
        In m3api, the main interface to continuation is the &lt;code&gt;requestAndContinue()&lt;/code&gt; method,
        which returns an async generator.
        It’s typically used in a &lt;a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for-await...of"&gt;&lt;code&gt;for await&lt;/code&gt; loop&lt;/a&gt; like this:
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class="keyword"&gt;for&lt;/span&gt; &lt;span class="keyword"&gt;await&lt;/span&gt; ( &lt;span class="storage type"&gt;const&lt;/span&gt; response of session.&lt;span class="function call"&gt;requestAndContinue&lt;/span&gt;( {
	action: &lt;span class="string"&gt;'query'&lt;/span&gt;,
	list: &lt;span class="string"&gt;'allpages'&lt;/span&gt;,
} ) ) {
	console.&lt;span class="function call"&gt;log&lt;/span&gt;( response );
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        Each &lt;code&gt;response&lt;/code&gt; is a response object like would be returned from a normal &lt;code&gt;request()&lt;/code&gt; call.
        You can &lt;code&gt;break;&lt;/code&gt; out of the loop at any time to stop making additional requests.
      &lt;/p&gt;
&lt;p&gt;
        The above example shows a “simple” case of continuation:
        each request produces one “batch” of pages (or, for some modules, revisions),
        and the next request continues with the next batch of different pages.
        However, it’s possible for a response to not contain the full data of one batch of pages.
        (An extreme example of this would be
        &lt;code&gt;action=query&lt;/code&gt;, &lt;code&gt;generator=querypage&lt;/code&gt;, &lt;code&gt;gqppage=Longpages&lt;/code&gt;, &lt;code&gt;gqplimit=500&lt;/code&gt;,
        &lt;code&gt;prop=revisions&lt;/code&gt;, &lt;code&gt;rvprop=text&lt;/code&gt; –
        that is, the text content of the 500 longest pages on the wiki.
        This will run into the response size limit very quickly,
        but the batch still contains all 500 longest pages,
        even though not all 500 are returned with their text in the same response.)
        In this case, continuation will first proceed &lt;em&gt;within&lt;/em&gt; one batch of pages
        (i.e., requests will return additional data for the same set of pages),
        and only proceed to the next batch after the full data for the previous batch has been returned,
        spread across multiple API responses.
        (It’s the caller’s responsibility to merge those responses back together again in a way that makes sense.)
        You can distinguish between these cases by the &lt;code&gt;batchcomplete&lt;/code&gt; member in the response:
        if it’s present (set to &lt;code&gt;""&lt;/code&gt; in &lt;code&gt;formatversion=1&lt;/code&gt; or &lt;code&gt;true&lt;/code&gt; in &lt;code&gt;formatversion=2&lt;/code&gt;),
        then the request returned the full set of data for the current batch of pages,
        and following continuation will proceed to the next batch;
        if it’s not present, then the request didn’t return the full data yet,
        and following continuation will yield additional data for the same batch of pages.
      &lt;/p&gt;
&lt;p&gt;
        m3api supports this distinction too, using the &lt;code&gt;requestAndContinueReducingBatch()&lt;/code&gt; method.
        It also returns an async generator,
        but follows continuation internally until the end of a batch has been reached,
        yielding a value that represents the combined result of all the responses for that batch.
        If you continue iterating over the async generator, it will continue with the next batch, and so on.
        When you use this method, you have to provide a &lt;code&gt;reducer()&lt;/code&gt; callback,
        which somehow merges the latest API response into the current accumulated value.
        The initial value for each batch can be specified via another callable,
        and otherwise defaults to &lt;code&gt;{}&lt;/code&gt; (empty object).
        This interface is similar to &lt;a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/reduce"&gt;&lt;code&gt;Array.reduce()&lt;/code&gt;&lt;/a&gt;
        (hence the name; elsewhere this operation is also known as &lt;a href="https://www.wikidata.org/wiki/Special:GoToLinkedPage/enwiki/Q951651"&gt;fold&lt;/a&gt;),
        but with a separate “reduction” taking place for each batch of pages returned by the API.
      &lt;/p&gt;
&lt;p&gt;
&lt;code&gt;requestAndContinueReducingBatch()&lt;/code&gt; is a fairly low-level method,
        and is not intended to be used directly.
        The &lt;a href="https://github.com/lucaswerkmeister/m3api-query/"&gt;m3api-query&lt;/a&gt; extension package offers some more convenient methods
        (assuming you’re using &lt;code&gt;action=query&lt;/code&gt;):
        &lt;code&gt;queryFullPageByTitle()&lt;/code&gt;, &lt;code&gt;queryFullPageByPageId()&lt;/code&gt; and &lt;code&gt;queryFullRevisionByRevisionId()&lt;/code&gt;
        return the full data for a single page or revision (even that can be split across multiple responses!),
        while &lt;code&gt;queryFullPages()&lt;/code&gt; and &lt;code&gt;queryFullRevisions()&lt;/code&gt;
        return async generators that yield full pages or revisions.
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class="keyword"&gt;for&lt;/span&gt; &lt;span class="keyword"&gt;await&lt;/span&gt; ( &lt;span class="storage type"&gt;const&lt;/span&gt; page of &lt;span class="function call"&gt;queryFullPages&lt;/span&gt;( session, {
	action: &lt;span class="string"&gt;'query'&lt;/span&gt;,
	list: &lt;span class="string"&gt;'allpages'&lt;/span&gt;,
} ) ) {
	console.&lt;span class="function call"&gt;log&lt;/span&gt;( page );
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        You get a simple, flat stream of pages,
        and don’t have to care that some of them may have been returned in the same response,
        others in a later response,
        and some may even have been split across multiple responses.
        The way in which pages from multiple responses are merged is configurable via the options,
        but the default should work for most cases.
        This is one of the parts of m3api I’m proudest of –
        making it easy to correctly work with API continuation.
      &lt;/p&gt;
&lt;h2 id="combining-requests"&gt;Combining requests&lt;/h2&gt;
&lt;p&gt;
        Another m3api feature I’m proud of is automatically combining concurrent compatible requests.
        The idea is taken from the &lt;a href="https://www.wikidata.org/wiki/Special:MyLanguage/Wikidata:Wikidata_Bridge"&gt;Wikidata Bridge&lt;/a&gt;
        (an interface to edit Wikidata from Wikipedia),
        where the Wikidata team at Wikimedia Germany (that I’m a part of) implemented something similar.
        (I reimplemented the idea from scratch in m3api to avoid infringing any copyright.)
      &lt;/p&gt;
&lt;p&gt;
        The Wikidata Bridge needs to load a lot of information from the API when it initializes itself:
      &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://de.wikipedia.beta.wmflabs.org/w/api.php?action=query&amp;amp;format=json&amp;amp;titles=Data-Bridge&amp;amp;prop=info&amp;amp;intestactions=edit&amp;amp;intestactionsdetail=full&amp;amp;errorformat=raw&amp;amp;formatversion=2"&gt;Whether the user has permission to edit the Wikipedia article.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://de.wikipedia.beta.wmflabs.org/w/api.php?action=query&amp;amp;format=json&amp;amp;meta=siteinfo&amp;amp;siprop=restrictions&amp;amp;errorformat=raw&amp;amp;formatversion=2"&gt;The Wikipedia site’s restriction levels, to determine what kind of protection the article has.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wikidata.beta.wmflabs.org/w/api.php?action=query&amp;amp;format=json&amp;amp;errorformat=raw&amp;amp;formatversion=2&amp;amp;titles=Q11&amp;amp;prop=info&amp;amp;intestactions=edit&amp;amp;intestactionsdetail=full"&gt;Whether the user has permission to edit the Wikidata item.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wikidata.beta.wmflabs.org/w/api.php?action=query&amp;amp;format=json&amp;amp;meta=siteinfo&amp;amp;errorformat=raw&amp;amp;formatversion=2&amp;amp;siprop=restrictions"&gt;The Wikidata site’s restriction levels, to determine what kind of protection the item has.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wikidata.beta.wmflabs.org/w/api.php?action=query&amp;amp;format=json&amp;amp;meta=siteinfo&amp;amp;errorformat=raw&amp;amp;formatversion=2&amp;amp;siprop=autocreatetempuser"&gt;Whether the Wikidata site has temporary accounts enabled, to determine whether to show a “your IP address will be publicly visible” warning.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wikidata.beta.wmflabs.org/w/api.php?action=query&amp;amp;format=json&amp;amp;meta=wbdatabridgeconfig&amp;amp;errorformat=raw&amp;amp;formatversion=2"&gt;The bridge configuration on Wikidata.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wikidata.beta.wmflabs.org/w/api.php?action=wbgetentities&amp;amp;format=json&amp;amp;props=datatype&amp;amp;ids=P443&amp;amp;errorformat=raw&amp;amp;formatversion=2"&gt;The data type of the property of the statement being edited.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wikidata.beta.wmflabs.org/w/api.php?action=wbgetentities&amp;amp;format=json&amp;amp;props=info&amp;amp;ids=Q11&amp;amp;errorformat=raw&amp;amp;formatversion=2"&gt;The latest revision ID of the item being edited.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wikidata.beta.wmflabs.org/w/api.php?action=wbgetentities&amp;amp;format=json&amp;amp;props=claims&amp;amp;ids=Q11&amp;amp;errorformat=raw&amp;amp;formatversion=2"&gt;The statements of the item being edited.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wikidata.beta.wmflabs.org/w/api.php?action=wbgetentities&amp;amp;format=json&amp;amp;props=labels&amp;amp;ids=P443&amp;amp;languages=de&amp;amp;languagefallback=true&amp;amp;errorformat=raw&amp;amp;formatversion=2"&gt;The label of the property of the statement being edited.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;
        A naïve implementation would make up to ten separate API requests to get this information
        (I’ve linked them above for the &lt;a href="https://de.wikipedia.beta.wmflabs.org/wiki/Data-Bridge"&gt;Beta Wikidata Bridge demo page&lt;/a&gt;).
        However, due to how API modules are designed to be flexible in which data they return,
        and how parameters that specify “I’d like &lt;em&gt;this&lt;/em&gt; piece of data” are often multi-valued,
        you can also combine them into just three requests:
        &lt;a href="https://de.wikipedia.beta.wmflabs.org/w/api.php?action=query&amp;amp;format=json&amp;amp;titles=Data-Bridge&amp;amp;prop=info&amp;amp;meta=siteinfo&amp;amp;intestactions=edit&amp;amp;intestactionsdetail=full&amp;amp;siprop=restrictions&amp;amp;errorformat=raw&amp;amp;formatversion=2"&gt;action=query on Wikipedia&lt;/a&gt; (1 and 2),
        &lt;a href="https://wikidata.beta.wmflabs.org/w/api.php?action=query&amp;amp;format=json&amp;amp;meta=wbdatabridgeconfig|siteinfo&amp;amp;errorformat=raw&amp;amp;formatversion=2&amp;amp;titles=Q11&amp;amp;prop=info&amp;amp;intestactions=edit&amp;amp;intestactionsdetail=full&amp;amp;siprop=autocreatetempuser|restrictions"&gt;action=query on Wikidata&lt;/a&gt; (3 to 6),
        and &lt;a href="https://wikidata.beta.wmflabs.org/w/api.php?action=wbgetentities&amp;amp;format=json&amp;amp;props=labels%7Cdatatype%7Cinfo%7Cclaims&amp;amp;ids=P443%7CQ11&amp;amp;languages=de&amp;amp;languagefallback=true&amp;amp;errorformat=raw&amp;amp;formatversion=2"&gt;action=wbgetentities on Wikidata&lt;/a&gt; (7 to 10).
        The simple approach to implement the initialization with just three requests
        would be to have one big blob of code that makes all the requests and extracts all the information from the responses,
        but this wouldn’t be very readable or maintainable:
        we’d rather have a bunch of &lt;a href="https://gerrit.wikimedia.org/g/mediawiki/extensions/Wikibase/+/a8f78a9456/client/data-bridge/src/data-access/ApiEntityLabelRepository.ts"&gt;smaller&lt;/a&gt;,
        &lt;a href="https://gerrit.wikimedia.org/g/mediawiki/extensions/Wikibase/+/a8f78a9456/client/data-bridge/src/data-access/ApiPropertyDataTypeRepository.ts"&gt;self-contained&lt;/a&gt;
&lt;a href="https://gerrit.wikimedia.org/g/mediawiki/extensions/Wikibase/+/a8f78a9456/client/data-bridge/src/data-access/ApiReadingEntityRepository.ts"&gt;services&lt;/a&gt;
        that each just specify the request parameters they need and extract the parts of the response that concern them.
        But how do we then combine those requests?
      &lt;/p&gt;
&lt;p&gt;
        One approach I’ve used in the Wikidata Image Positions tool (written in Python)
        is to explicitly split the API requests into three “phases”: assemble the parameters, make the request, process the response.
        Then you can assemble the parameters from multiple requests, make only one request, and process the same response multiple times
        (example based on &lt;a href="https://gitlab.wikimedia.org/toolforge-repos/wd-image-positions/-/blob/b8022cddca/app.py#L697"&gt;&lt;code&gt;load_image()&lt;/code&gt;&lt;/a&gt;):
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;query_params &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="function call"&gt;query_default_params&lt;/span&gt;()
&lt;span class="function call"&gt;image_attribution_query_add_params&lt;/span&gt;(
    query_params,
    image_title,
)
&lt;span class="function call"&gt;image_size_query_add_params&lt;/span&gt;(
    query_params,
    image_title,
)

query_response &lt;span class="keyword operator"&gt;=&lt;/span&gt; session.&lt;span class="function call"&gt;get&lt;/span&gt;(&lt;span class="keyword operator"&gt;*&lt;/span&gt;&lt;span class="keyword operator"&gt;*&lt;/span&gt;query_params)

attribution &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="function call"&gt;image_attribution_query_process_response&lt;/span&gt;(
    query_response,
    image_title,
)
width, height &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="function call"&gt;image_size_query_process_response&lt;/span&gt;(
    query_response,
    image_title,
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        But this is fairly cumbersome,
        and also requires the calling code to know which requests can be combined and which can’t.
        We can do better.
      &lt;/p&gt;
&lt;p&gt;
        Because all requests are asynchronous in JavaScript, &lt;!-- please do not @ me about sync XHR --&gt;
        our &lt;code&gt;request()&lt;/code&gt; function can return a &lt;code&gt;Promise&lt;/code&gt; without immediately making an underlying network request.
        We can then wait for a very short period
        (specifically, until the next &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/HTML_DOM_API/Microtask_guide"&gt;microtask&lt;/a&gt;),
        and see if any other requests come in during that time;
        if they do, we check if they’re compatible, and potentially merge them into the pending request.
        Then, we send the pending request(s),
        and resolve the associated promises with the response(s).
      &lt;/p&gt;
&lt;p&gt;
        The effect of this is that,
        when several compatible requests are made within the same JS event loop run,
        then m3api can merge them automatically.
        Most often, making several requests within the same JS event loop run looks like a call to &lt;code&gt;Promise.all()&lt;/code&gt; with several requests
        (see the example below).
      &lt;/p&gt;
&lt;p&gt;
        To determine whether requests are compatible,
        we need to distinguish between list-type parameters that can be merged,
        and ones that can’t be.
        The convention we used in the Wikidata Bridge,
        and which I reused for m3api,
        is that mergeable parameters are specified as &lt;code&gt;Set&lt;/code&gt;s,
        while unmergeable parameters are specified as &lt;code&gt;Array&lt;/code&gt;s.
        (The reasoning behind this is that, in many other languages, sets are unordered,
        and when a parameter is mergeable then you probably don’t care about the order the parameters are sent in;
        conversely, when you care about the order, you probably don’t want another request’s values to be inserted in front of yours.
        This doesn’t 100% apply in JavaScript because &lt;code&gt;Set&lt;/code&gt;s obey insertion order,
        but I think it still makes some sense.)
        So, two requests are compatible if all their parameters either only occur in one request
        (e.g. one has &lt;code&gt;list=allpages&lt;/code&gt; while the other has &lt;code&gt;meta=siteinfo&lt;/code&gt;),
        have the same value in both requests
        (e.g. both have &lt;code&gt;action=query&lt;/code&gt;),
        or are specified as &lt;code&gt;Set&lt;/code&gt; in both requests.
        To make creating &lt;code&gt;Set&lt;/code&gt;s more convenient,
        a &lt;code&gt;set()&lt;/code&gt; helper function is provided,
        so that e.g. requests with &lt;code&gt;list: set( 'allpages' )&lt;/code&gt; and &lt;code&gt;list: set( 'allusers' )&lt;/code&gt; are compatible.
      &lt;/p&gt;
&lt;p&gt;
        The upshot of this is that the following example code will only make one underlying network request,
        with &lt;code&gt;siprop=general|statistics&lt;/code&gt;:
      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class="keyword"&gt;async&lt;/span&gt; &lt;span class="storage function"&gt;function&lt;/span&gt; &lt;span class="entity name function"&gt;getSiteName&lt;/span&gt;( session ) {
	&lt;span class="storage type"&gt;const&lt;/span&gt; response &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="keyword"&gt;await&lt;/span&gt; session.&lt;span class="function call"&gt;request&lt;/span&gt;( {
		action: &lt;span class="string"&gt;'query'&lt;/span&gt;,
		meta: &lt;span class="function call"&gt;set&lt;/span&gt;( &lt;span class="string"&gt;'siteinfo'&lt;/span&gt; ),
		siprop: &lt;span class="function call"&gt;set&lt;/span&gt;( &lt;span class="string"&gt;'general'&lt;/span&gt; ),
	} );
	&lt;span class="keyword"&gt;return&lt;/span&gt; response.query.general.sitename;
}

&lt;span class="keyword"&gt;async&lt;/span&gt; &lt;span class="storage function"&gt;function&lt;/span&gt; &lt;span class="entity name function"&gt;getSiteEdits&lt;/span&gt;( session ) {
	&lt;span class="storage type"&gt;const&lt;/span&gt; response &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="keyword"&gt;await&lt;/span&gt; session.&lt;span class="function call"&gt;request&lt;/span&gt;( {
		action: &lt;span class="string"&gt;'query'&lt;/span&gt;,
		meta: &lt;span class="function call"&gt;set&lt;/span&gt;( &lt;span class="string"&gt;'siteinfo'&lt;/span&gt; ),
		siprop: &lt;span class="function call"&gt;set&lt;/span&gt;( &lt;span class="string"&gt;'statistics'&lt;/span&gt; ),
	} );
	&lt;span class="keyword"&gt;return&lt;/span&gt; response.query.statistics.edits;
}

&lt;span class="storage type"&gt;const&lt;/span&gt; [ sitename, edits ] &lt;span class="keyword operator"&gt;=&lt;/span&gt; &lt;span class="keyword"&gt;await&lt;/span&gt; &lt;span class="support class promise"&gt;Promise&lt;/span&gt;.&lt;span class="function call"&gt;all&lt;/span&gt;( [
	&lt;span class="function call"&gt;getSiteName&lt;/span&gt;( session ),
	&lt;span class="function call"&gt;getSiteEdits&lt;/span&gt;( session ),
] );&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
        In principle, it’s possible that automatically combining requests will cause bugs in code written by developers who aren’t aware of this m3api feature.
        (For example, if someone doesn’t use m3api-query,
        they might use code like &lt;code&gt;response.query.pages[ 0 ]&lt;/code&gt; to access the only page they expect to be present in the response,
        without realizing that a merged request may have caused further pages to be returned.)
        However, I hope that this will be rare,
        thanks to the combination of requests only being combined if they happen within the same JS event loop run
        and array-type parameters not being eligible for combining.
        If I get a lot of bug reports about this feature,
        I may reconsider it for the next major version.
        (If you want to make absolutely sure that a particular request will not be combined with any other,
        specify the &lt;code&gt;action&lt;/code&gt; as a single-element array,
        e.g. &lt;code&gt;action: [ 'query' ]&lt;/code&gt; –
        every other request will also specify the &lt;code&gt;action&lt;/code&gt; parameter,
        and they’ll all be incompatible,
        because arrays are not mergeable.)
      &lt;/p&gt;
&lt;h2 id="error-handling"&gt;Error handling&lt;/h2&gt;
&lt;p&gt;
        As you might expect, m3api detects errors in the response and throws them
        (or, if you prefer, it rejects the promise, because all of this is async).
        As you might also expect, any warnings in the response are detected and, by default, logged to the console via &lt;code&gt;console.warn()&lt;/code&gt;.
        (I was actually surprised to discover the other day that MediaWiki’s own &lt;code&gt;mw.Api()&lt;/code&gt; doesn’t do this.
        God knows how many on-wiki gadgets and user scripts use deprecated API parameters without realizing it because the warnings returned by the API go straight to &lt;code&gt;/dev/null&lt;/code&gt;…)
      &lt;/p&gt;
&lt;p&gt;
        m3api also supports transparently handling errors without throwing them.
        Several errors returned by the API can be handled by retrying the request in some form;
        m3api’s approach is to retry requests until a certain time limit (by default, 65 seconds) after the initial request has passed –
        I think this makes more sense than limiting the absolute number of retries, as some other libraries do.
        (You can change the limit using the &lt;code&gt;maxRetriesSeconds&lt;/code&gt; request option –
        bots may want to use a much longer limit than interactive applications.)
        If the response by the API includes a &lt;code&gt;Retry-After&lt;/code&gt; header, m3api will obey it (as long as it’s within said time limit);
        otherwise, error handlers for different error codes can be configured,
        which may likewise retry the request.
        m3api ships error handlers for &lt;code&gt;badtoken&lt;/code&gt; (update the token, then retry),
        &lt;code&gt;maxlag&lt;/code&gt; and &lt;code&gt;readonly&lt;/code&gt; errors (sleep for an appropriate time period, then retry).
        The &lt;a href="https://github.com/lucaswerkmeister/m3api-oauth2/"&gt;m3api-oauth2&lt;/a&gt; extension package
        installs an error handler to refresh expired OAuth 2 access tokens
        (on Wikimedia wikis, they expire after 4 hours)
        and then retry the request.
        These retries are always transparent to the code that made the request.
      &lt;/p&gt;
&lt;h2 id="why-you-should-use-it"&gt;Why you should use it&lt;/h2&gt;
&lt;p&gt;
        I’m of course biased, but I happen to think it’s a well-designed library, for various reasons including the ones detailed above ;)
        but I’ll close by mentioning some of the recommendations in the &lt;a href="https://www.mediawiki.org/wiki/API:Etiquette"&gt;API Etiquette&lt;/a&gt;
        (&lt;a href="https://www.mediawiki.org/w/index.php?title=API:Etiquette&amp;amp;oldid=7556535"&gt;permalink&lt;/a&gt;)
        and outlining how m3api aligns with them:
      &lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;request limit&lt;/dt&gt;
&lt;dd&gt;
          This is partially up to the developer using m3api, but m3api supports “ask[ing] for multiple items in one request”,
          both manually by specifying parameters as lists or sets (e.g. &lt;code&gt;titles: set( 'PageA', 'PageB', 'PageC' )&lt;/code&gt;)
          and automatically by combining requests as explained above.
          Also, as mentioned in the error handling section,
          &lt;code&gt;Retry-After&lt;/code&gt; response headers are respected;
          this isn’t explicitly mentioned on the API Etiquette page, but I’ve heard it’s still considered good bot practice.
        &lt;/dd&gt;
&lt;dt&gt;maxlag&lt;/dt&gt;
&lt;dd&gt;
          Specifying the maxlag parameter is up to the developer using m3api,
          but m3api &lt;a href="https://github.com/lucaswerkmeister/m3api/?tab=readme-ov-file#recommendations-for-bots"&gt;recommends it&lt;/a&gt; for bots,
          and if it is used, then m3api will automatically wait and retry the request if the API returns a maxlag error.
        &lt;/dd&gt;
&lt;dt&gt;User-Agent header&lt;/dt&gt;
&lt;dd&gt;
          m3api sends a general User-Agent header for itself by default,
          and also &lt;a href="https://github.com/lucaswerkmeister/m3api/?tab=readme-ov-file#usage-recommendations"&gt;encourages&lt;/a&gt; developers to specify a custom User-Agent header.
          If developers neglect to specify the &lt;code&gt;userAgent&lt;/code&gt; request option,
          a warning is logged (by default, to &lt;code&gt;console.warn()&lt;/code&gt;, where it should be relatively visible).
        &lt;/dd&gt;
&lt;dt&gt;data formats&lt;/dt&gt;
&lt;dd&gt;
          m3api uses the JSON format (of course).
        &lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;
        If you’re already using a different API library or framework,
        you’re free to continue using it, naturally.
        But if you’re currently making network requests to the API directly,
        or if you’re going to start a new project where you need to interact with the API,
        I encourage you to give m3api a try.
        And if you use it, please let me know how it’s working for you!
      &lt;/p&gt;

&lt;/article&gt;</description><guid isPermaLink="true">https://lucaswerkmeister.de/posts/2025/04/12/introducing-m3api/</guid><pubDate>Sat, 12 Apr 2025 00:00:00 GMT</pubDate></item><item><title>Archiving my tweets</title><link>https://lucaswerkmeister.de/posts/2025/02/01/archiving-my-tweets/</link><description>&lt;article&gt;

&lt;p&gt;
        As you may be aware, Twitter has been going down the toilet for a while,
        especially since noted asshole Jack Dorsey sold it to noted even bigger asshole Elon Musk.
        As you may also be aware, I used to use Twitter quite a bit,
        and I also generally enjoy referring back to my older social media posts.
        So while I moved to Mastodon a while ago
        (first the now-defunct mastodon.technology, then &lt;a href="https://wikis.world/"&gt;Wikis World&lt;/a&gt;),
        I would still like to have access to my old tweets.
      &lt;/p&gt;
&lt;p&gt;
        After opening tabs for several Twitter archivers and then not doing anything with them for ca. two years,
        I’ve now finally tried them out, picked one I liked, and started setting it up.
        My archiver of choice is &lt;a href="https://tinysubversions.com/twitter-archive/make-your-own/"&gt;Darius Kazemi aka Tiny Subversions’ simple Twitter archiver&lt;/a&gt;,
        which is very easy to use (it runs in the browser: drop in your ZIP file, get a new ZIP file out),
        fast, and produces output that I find useful and pleasing.
        Its URL format also lends itself to supporting multiple archives next to each other,
        which should be useful later;
        I only had to tweak the &lt;code&gt;app.js&lt;/code&gt; and &lt;code&gt;index.html&lt;/code&gt; files slightly to make this work.
      &lt;/p&gt;
&lt;p&gt;
        You can find the @LucasWerkmeistr Twitter archive at &lt;strong&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/"&gt;twitter.lucaswerkmeister.de/LucasWerkmeistr/&lt;/a&gt;&lt;/strong&gt;,
        and individual tweets at URLs like &lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1154827181387898882/"&gt;twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1154827181387898882&lt;/a&gt;,
        easily rewritable from the original URL &lt;a href="https://twitter.com/LucasWerkmeistr/status/1154827181387898882"&gt;twitter.com/LucasWerkmeistr/status/1154827181387898882&lt;/a&gt; –
        just replace the &lt;code&gt;twitter.com&lt;/code&gt; domain with &lt;code&gt;twitter.lucaswerkmeister.de&lt;/code&gt;.
        The archive also includes a search feature;
        I’m &lt;em&gt;slightly&lt;/em&gt; nervous about someone using it to find stupid things I wrote years ago,
        but I think on the whole it should be okay as long as people keep in mind that this is an archive of things I posted in the past,
        not necessarily an enthusiastic endorsement that I would post all of it in exactly the same way again today.
      &lt;/p&gt;
&lt;p&gt;
        The biggest shortcoming of this Twitter archiver, which it shares with the other options I tried,
        is that it doesn’t include the alt text of the images I posted.
        This is very unfortunate, as the alt text is important for accessibility,
        and also I put a lot of work into those alt texts if you add it up and I don’t want to lose it.
        (The alt text is not included in Twitter’s own exports / archives,
        so any tool that’s based on them is going to have the same limitation.
        I assume in principle it would be possible for someone to build a tool that fetches the alt text from Twitter now,
        as long as the tweets haven’t been deleted yet,
        but I don’t know if anyone’s done that and I’m certainly not interested in switching to a different archiver now.)
        So I picked out my most popular tweets – using the command
        &lt;code&gt;sed 's/^const searchDocuments = //' searchDocuments.js | jq '[.[] | select(.full_text | contains("tweets_media"))] | sort_by(.favorite_count | tonumber | - .) | .[]' | less&lt;/code&gt;
        which picks them out of the search data – and manually copied the alt text for those,
        as well as for plenty of other tweet threads where I wanted to keep the alt text,
        just by copying it from the existing tweets (which I haven’t deleted yet)
        and hand-editing it into the HTML files.
        (Emacs’ syntax highlighting tells me when I need to switch between &lt;code&gt;alt=""&lt;/code&gt; and &lt;code&gt;alt=''&lt;/code&gt;, and/or HTML-escape individual single quotes as &lt;code&gt;&amp;amp;#39;&lt;/code&gt;, because of quote characters in the alt text.)
        I expect I’ll keep doing this for a few more days after this blog post goes up,
        and once I decide I’ve copied enough alt text,
        then it will probably finally be time for me to, at long last, delete my account.
      &lt;/p&gt;
&lt;p&gt;
        (Side note one: as the archive shows the full thread of a tweet on each page,
        but still has a separate page per tweet,
        threaded tweets are included in several copies – once per thread in the tweet.
        I’ve generally only manually added the alt text to the page for the “top” tweet in the thread –
        for instance, &lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1164116092882739200/"&gt;this page&lt;/a&gt; has the alt text for &lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1164145887381245952/"&gt;this tweet&lt;/a&gt; –
        so if you ended up on a page in the middle of a thread, you might want to follow the link at the top.
        If I can be bothered, maybe I’ll later write a little program that syncs the &lt;code&gt;alt=&lt;/code&gt; attributes between different pages.
        [2025-02-09 update: I could be bothered. See below.])
      &lt;/p&gt;
&lt;p&gt;
        (Side note two: in the original output of the archiver,
        links to other tweets in the tweets themselves weren’t updated;
        I fixed the links pointing to my own tweets using the command
        &lt;code&gt;sed -i.sedbak -E 's|&amp;lt;a href="https://twitter.com/LucasWerkmeistr/status/([1-9][0-9]*)"&amp;gt;https://twitter.com/LucasWerkmeistr/status/\1&amp;lt;/a&amp;gt;|&amp;lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/\1"&amp;gt;https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/\1&amp;lt;/a&amp;gt;|g' */index.html&lt;/code&gt; –
        i.e. making them point back to the archive rather than the original URL.
        If anyone else I’ve interacted with has also archived their tweets in a similar way,
        let me know and I can make my archived tweets point to your archive 🎉)
      &lt;/p&gt;
&lt;p&gt;
        As part of copying the alt text, I also rediscovered many of my threads that I liked,
        so here’s a selection of some of them –
        some of my favorite tweets and threads:
      &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1154826541588766726/"&gt;my coming out!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/824357495397289991/"&gt;systemd tip of the day&lt;/a&gt; (&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/849377832866377730/"&gt;part 2&lt;/a&gt;), a series I did for a while (I assume most of the tips should still apply but do keep in mind they’re ca. eight years old 😄)&lt;/li&gt;
&lt;li&gt;Wikimedia tools:
          &lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1006863422569492481/"&gt;Wikidata Lexeme Forms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1054066169907425280/"&gt;Wikidata Image Positions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1211216978083491841/"&gt;SpeedPatrolling&lt;/a&gt; (&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1180579346966138880/"&gt;alternate post&lt;/a&gt;; see also the &lt;a href="https://lucaswerkmeister.de/posts/2019/01/04/speedpatrolling/"&gt;behind-the-scenes blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1107400603649609729/"&gt;QuickCategories&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AC/DC: no central thread, but see &lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1171886943375900672/"&gt;gadget announcement&lt;/a&gt;, &lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1441133597365358592/"&gt;easter egg&lt;/a&gt;, &lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1491556600561254404/"&gt;new features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1355554819478532102/"&gt;Ranker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pride parades:
          &lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1023258024079699968/"&gt;Berlin 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1155071561105399808/"&gt;Berlin 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1408749926356602889/"&gt;Berlin 2021 (June)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1418889315606749187/"&gt;Berlin 2021 (July)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1550780652072615938/"&gt;Berlin 2022&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Big holiday / travel threads:
          &lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1161171648315367424/"&gt;Wikimania 2019&lt;/a&gt; – Berlin to Stockholm (see also Queryboo below)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1163655092400283649/"&gt;Wikimania 2019&lt;/a&gt; – Stockholm to Berlin [added 2025-02-13]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1164116092882739200/"&gt;CCCamp 2019&lt;/a&gt; – Mildenberg&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1299260745583034368/"&gt;Summer 2020&lt;/a&gt; – Berlin to Fürth&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1432239389380337664/"&gt;Summer 2021&lt;/a&gt; – southern Germany round trip&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1555635275572940805/"&gt;Summer 2022&lt;/a&gt; – Alps, France, United Kingdom (including bike tour there); this one was split across multiple threads but they’re all linked to each other&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1161176506418249728/"&gt;Queryboo travel thread&lt;/a&gt; (concurrent with Wikimania 2019 linked above)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1395790502809804800/"&gt;Wikimedia Hackathon 2021&lt;/a&gt; (remote event)&lt;/li&gt;
&lt;li&gt;Covid vaccine selfies:
          &lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1405423730088624129/"&gt;June 2021&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1423263773499039749/"&gt;August 2021&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1473626033077301250/"&gt;December 2021&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1530110423302815744/"&gt;May 2022&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Shorter bike tours (for longer ones, see the travel threads above):
          &lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1242396608656027648/"&gt;March 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1267063408261763072/"&gt;May 2020&lt;/a&gt; (&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1267114176054525954/"&gt;part 2&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1284823454936715266/"&gt;July 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1294963724609097728/"&gt;16 August 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1297103431912558593/"&gt;22 August 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1423980748374224897/"&gt;August 2021 (post-vaccine)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1516052902053396481/"&gt;April 2022&lt;/a&gt; (short but I still really like the joke in the second post ^^)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1530867014700519424/"&gt;May 2022 (post-vaccine)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1533747147673374720/"&gt;June 2022&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1351536421086121986/"&gt;first(?) appearance of the “fuck biphobia” sweater&lt;/a&gt; – this would turn out to be my most-faved tweet, heh&lt;/li&gt;
&lt;li&gt;Painted nails / nail polish / #nailart:
          &lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1187708195717943296/"&gt;25 October 2019&lt;/a&gt; (WikidataCon 2019)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1187747443959566339/"&gt;25 October 2019&lt;/a&gt; (WikidataCon 2019), feat. Harmonia Amanda&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1193900699177963520/"&gt;11 November 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1201456320332881922/"&gt;2 December 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1205113655647506432/"&gt;12 December 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1210489539891998721/"&gt;19 December 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1212075355156230147/"&gt;31 December 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1219740069155745792/"&gt;21 January 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1221489790589329408/"&gt;26 January 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1226584679849000960/"&gt;9 February 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1233715852077301762/"&gt;29 February 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1244711490269204484/"&gt;30 March 2020&lt;/a&gt;, the first one under the influence of covid apparently&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1262716886036492289/"&gt;19 May 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1405423730088624129/"&gt;17 June 2021&lt;/a&gt; (also linked under the vaccine selfies above)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1280271600147972102/"&gt;7 July 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1293936669012504578/"&gt;13 August 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1298959741775941633/"&gt;27 August 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1320400758270021632/"&gt;25 October 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1358145980177219588/"&gt;6 February 2021&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1454004321301970949/"&gt;29 October 2021&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1457338310758645763/"&gt;7 November 2021&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.lucaswerkmeister.de/LucasWerkmeistr/status/1381731582545911811/"&gt;gratuitous selfie thread&lt;/a&gt; (includes nail polish, 13 April 2021) – did you know I had a beard for a while during covid? wild&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
        As I mentioned, the archive’s URL format can accommodate multiple Twitter accounts’ archives on the same domain,
        so I expect at some point (hopefully soon) I’ll also set up twitter.lucaswerkmeister.de/WikidataFacts/ and twitter.lucaswerkmeister.de/ItsBiNotHetero/ –
        stay tuned 🙂
      &lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;2025-02-09 update:&lt;/strong&gt; I wrote the program to synchronize the alt texts:
        it’s called &lt;a href="https://github.com/lucaswerkmeister/sync-alt"&gt;sync-alt&lt;/a&gt;
        and I applied it to the Twitter archive,
        so now all the posts where I copied the alt text over should have it,
        regardless of where in the thread you’re viewing them.
      &lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;2025-03-10 update:&lt;/strong&gt; All my tweets should have their alt text copied now
        (except for the ones that didn’t have alt text;
        I sometimes left HTML comments about such cases but those aren’t synced between copies).
        Also, the &lt;a href="https://twitter.lucaswerkmeister.de/ItsBiNotHetero/"&gt;@ItsBiNotHetero archive&lt;/a&gt; is now available,
        and I updated my tweets that pointed to ItsBiNotHetero tweets to link to the archive instead
        (using a similar &lt;code&gt;sed&lt;/code&gt; command as when updating the links to my own tweets, as mentioned above).
      &lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;2025-03-14 update:&lt;/strong&gt; The &lt;a href="https://twitter.lucaswerkmeister.de/WikidataFacts/"&gt;@WikidataFacts archive&lt;/a&gt; is now also available,
        with copied alt texts (as long as the original tweets had alt text –
        tweets before ca. June/July 2016 lacked them),
        and I also updated cross-links between @LucasWerkmeistr and @WikidataFacts tweets to point to the archives.
      &lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;2025-03-15 update:&lt;/strong&gt; The &lt;a href="https://twitter.lucaswerkmeister.de/AfDimBundesrat/"&gt;@AfDimBundesrat archive&lt;/a&gt; is now also available.
        This is a parody account – I noticed one day that the handle was free and grabbed it on a whim;
        now, if (heaven forbid) the &lt;a href="https://en.wikipedia.org/wiki/Alternative_for_Germany"&gt;AfD&lt;/a&gt; should ever gain seats in the &lt;a href="https://en.wikipedia.org/wiki/German_Bundesrat"&gt;Bundesrat&lt;/a&gt;,
        the most obvious handle for them won’t be available.
        I occasionally posted variations of “the AfD isn’t in the Bundesrat” there just so nobody could usurp the account on the basis of it being unused,
        and I might still keep doing that for the time being.
        (I never moved this account to the fediverse, because it only makes sense in a single global namespace like Twitter.)
      &lt;/p&gt;

&lt;/article&gt;</description><guid isPermaLink="true">https://lucaswerkmeister.de/posts/2025/02/01/archiving-my-tweets/</guid><pubDate>Sat, 01 Feb 2025 00:00:00 GMT</pubDate></item><item><title>Reduce The Size Of Your Tarsnap Backups With This One Weird Trick (the trick is “pay attention to the size of your backups”)</title><link>https://lucaswerkmeister.de/posts/2024/12/10/tarsnap-backup-size/</link><description>&lt;article&gt;

&lt;p&gt;
&lt;a href="https://www.tarsnap.com/"&gt;Tarsnap&lt;/a&gt; is an encrypted backup product.
        The client software is gratis and source-available (but not FLOSS);
        you pay for storage (US$ 0.25 / GB-month) and network traffic (US$ 0.25 / GB) on and with their service.
        You get an email notification when your current account balance dips below 7 days’ worth of storage costs at your current size,
        at which point you should increase your balance and/or delete some old backups.
        (There’s no built-in auto-deletion of backups;
        some third-party software exists but I just do it manually.)
        My backups include &lt;code&gt;/etc&lt;/code&gt; and &lt;code&gt;/home&lt;/code&gt;,
        with an exclude list of various files and directories that I don’t need backed up
        which I put together when I first set up these backups.
      &lt;/p&gt;
&lt;p&gt;
        When I most recently got this notification,
        I was disturbed to see that the daily storage costs were higher than usual –
        not exorbitant but bad enough to warrant inspection.
        I could see in the account history on which day the storage cost had increased,
        but I didn’t remember what happened on that day which might have triggered it.
        I set out to extract the most recent backup so I could inspect it with &lt;code&gt;ncdu&lt;/code&gt; –
        this seemed like the most practical way to find out what was taking up the most space inside.
        The extraction process took much longer than I expected
        (as of this writing, it’s been running for 10 days and hasn’t actually finished yet),
        but partway through it became clear that I had inadvertently included a large SQL dump in the backup,
        and failed to update the exclude list when renaming some previously-excluded ISO files.
      &lt;/p&gt;
&lt;p&gt;
        Because waiting for days to extract the latest backup doesn’t sound like a great experience to go through regularly
        (and remember, you’re paying for that network traffic to download it again!),
        I started thinking about alternative solutions to find out what’s getting backed up.
        I found out that &lt;code&gt;&lt;a href="https://en.wikipedia.org/wiki/Ncdu"&gt;ncdu&lt;/a&gt;&lt;/code&gt; (“ncurses disk usage”) has an option to exclude files from the disk usage report,
        and its pattern syntax is (as far as I can tell) compatible with the one for Tarsnap.
        So I put together a little shell script to run &lt;code&gt;ncdu&lt;/code&gt; with the exclude list from my Tarsnap config;
        you can find the &lt;a href="https://github.com/lucaswerkmeister/home/blob/main/.bashrc.d/ncdu-tarsnap"&gt;latest version&lt;/a&gt; on GitHub (assuming I don’t rename the file),
        or the current version as of this writing below:
      &lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;&lt;code&gt;ncdu-tarsnap&lt;/code&gt;&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class="comment"&gt;# Show disk usage of tarsnap backups.&lt;/span&gt;
&lt;span class="comment"&gt;#&lt;/span&gt;
&lt;span class="comment"&gt;# You can delete files in ncdu,&lt;/span&gt;
&lt;span class="comment"&gt;# but keep in mind that ncdu is operating on the local file system.&lt;/span&gt;
&lt;span class="comment"&gt;# Don’t delete any files that you want to keep there,&lt;/span&gt;
&lt;span class="comment"&gt;# and don’t assume that they will be removed from any existing backups.&lt;/span&gt;
&lt;span class="comment"&gt;#&lt;/span&gt;
&lt;span class="comment"&gt;# Assumptions that apply to my setup but may or may not apply to others’:&lt;/span&gt;
&lt;span class="comment"&gt;# * the most relevant folder being backed up is ~&lt;/span&gt;
&lt;span class="comment"&gt;#   (I actually back up /home and /etc but everything outside of ~ is negligible –&lt;/span&gt;
&lt;span class="comment"&gt;#   ncdu doesn’t support inspecting multiple directories at once)&lt;/span&gt;
&lt;span class="comment"&gt;# * some files below ~ are not $USER-readable, so running ncdu with sudo is useful&lt;/span&gt;
&lt;span class="comment"&gt;# * the backup is being made as root&lt;/span&gt;
&lt;span class="comment"&gt;#   (otherwise the non-$USER-readable files should not be counted after all)&lt;/span&gt;
&lt;span class="comment"&gt;# * only /etc/tarsnap/tarsnap.conf is used&lt;/span&gt;
&lt;span class="comment"&gt;#   (no ~/.tarsnaprc and also no --exclude on the command line)&lt;/span&gt;
&lt;span class="keyword"&gt;function&lt;/span&gt; ncdu-tarsnap {
    &lt;span class="comment"&gt;# bash -c is needed because sudo … &amp;lt;() doesn’t work properly (see -C in sudo(8))&lt;/span&gt;

    &lt;span class="comment"&gt;# --apparent-size probably makes more sense for a backup than --disk-usage&lt;/span&gt;

    &lt;span class="comment"&gt;# note that this will also show (with empty size)&lt;/span&gt;
    &lt;span class="comment"&gt;# files that are excluded;&lt;/span&gt;
    &lt;span class="comment"&gt;# unfortunately, ncdu’s --hide-hidden hides both hidden (.*) and excluded files,&lt;/span&gt;
    &lt;span class="comment"&gt;# and there seems to be no option for hiding excluded but showing hidden files :(&lt;/span&gt;
    sudo bash -c &lt;span class="string"&gt;"ncdu \"&lt;/span&gt;$HOME\" --apparent-size --exclude-from &lt;span class="keyword operator"&gt;&amp;lt;&lt;/span&gt;(sed -n &lt;span class="string"&gt;'/^exclude\s\+/ s///p'&lt;/span&gt; /etc/tarsnap/tarsnap.conf)"
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;
        Looking through the data-that-would-be-backed-up in &lt;code&gt;ncdu-tarsnap&lt;/code&gt;,
        I was able to identify several patterns that I should add to my exclude list,
        and also some data that I could also just delete from my live file system.
        So that’s the first part of my One Weird Trick (so sorry about that title):
        &lt;strong&gt;adjust your exclude patterns based on your current file system contents.&lt;/strong&gt;
&lt;/p&gt;
&lt;p&gt;
        That’s all nice and well, but what happens the next time I leave a large SQL dump in my home directory without thinking about the backups?
        I realized I needed to set up a process to periodically check the total size of the data-that-would-be-backed-up and alert me if it got too large.
        &lt;code&gt;ncdu&lt;/code&gt; isn’t useful for that,
        but fortunately &lt;code&gt;&lt;a href="https://en.wikipedia.org/wiki/Du_(Unix)"&gt;du&lt;/a&gt;&lt;/code&gt; (“disk usage”) also has exclude-pattern support,
        again with compatible syntax,
        and is easy to use in a shell script.
        Sending a desktop notification from a systemd system service
        (I need it to run with privileges because not all the files in my home directory are readable by me)
        isn’t pretty, but the following works:
      &lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;&lt;code&gt;tarsnap-size-check.service&lt;/code&gt;&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;# /etc/systemd/system/tarsnap-size-check.service
[Unit]
Description=Warn Lucas if it appears that the next tarsnap backup would be bigger than 50 GB

[Service]
Type=oneshot
ExecStart=bash -s
StandardInputText=\
  bytes=$(du --summarize --bytes /home /etc --exclude-from &amp;lt;(sed -n '/^exclude\\s\\+/ s///p' /etc/tarsnap/tarsnap.conf) | awk '{ total += $1 } END { print total }'); \
  if (( bytes &amp;lt;= 50000000000 )); then exit 0; fi; \
  gigabytes=$(bc &amp;lt;&amp;lt;&amp;lt; "scale=1; $bytes/1000000000"); \
  DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/$UID/bus notify-send -a 'Tarsnap size warning' -i dialog-warning 'Tarsnap backup too large' "The projected size of a new backup is $gigabytes GB ($bytes bytes), should be below 50 GB! Use ncdu-tarsnap to inspect the situation."; \
  exit 1
User=lucas
AmbientCapabilities=CAP_DAC_READ_SEARCH

CapabilityBoundingSet=CAP_DAC_READ_SEARCH
IPAddressDeny=any
LockPersonality=yes
MemoryDenyWriteExecute=yes
NoNewPrivileges=yes
PrivateDevices=yes
PrivateMounts=yes
PrivateNetwork=yes
PrivateTmp=yes
ProtectClock=yes
ProtectControlGroups=yes
ProtectHome=read-only
ProtectHostname=yes
ProtectKernelLogs=yes
ProtectKernelModules=yes
ProtectKernelTunables=yes
ProtectProc=invisible
ProtectSystem=strict
RestrictAddressFamilies=AF_UNIX
RestrictNamespaces=yes
RestrictRealtime=yes
RestrictSUIDSGID=yes
SystemCallArchitectures=native
SystemCallFilter=@system-service
SystemCallFilter=~@privileged @resources
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;code&gt;tarsnap-size-check.timer&lt;/code&gt;&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;# /etc/systemd/system/tarsnap-size-check.timer
[Unit]
Description=Regularly monitor the size of tarsnap backups according to the current configuration

[Timer]
OnCalendar=hourly
AccuracySec=1h

[Install]
WantedBy=timers.target
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;
        (Of course, you may want to adjust the hard-coded threshold to something other than 50 GB.
        And maybe you prefer another notification setup as well.)
        The timer is set up to run hourly, whereas my backups run weekly,
        so that should give me enough time to act whenever it notifies me.
        (I included a pointer to &lt;code&gt;ncdu-tarsnap&lt;/code&gt; in the notification text in case I forget what I called the script.)
        And that’s the second part of my One Weird Trick:
        &lt;strong&gt;Set up alerts if your projected backup size exceeds a preconfigured limit.&lt;/strong&gt;
&lt;/p&gt;
&lt;p&gt;
        Either of these could be built into Tarsnap,
        but as far as I can tell, they aren’t.
        But at least it’s not too difficult to build them around Tarsnap instead.
        (The above scripts and units might even be applicable to other backup solutions,
        as long as those solutions also use &lt;a href="https://en.wikipedia.org/wiki/Glob_(programming)"&gt;glob/fnmatch&lt;/a&gt;-like patterns.)
      &lt;/p&gt;

&lt;/article&gt;</description><guid isPermaLink="true">https://lucaswerkmeister.de/posts/2024/12/10/tarsnap-backup-size/</guid><pubDate>Tue, 10 Dec 2024 00:00:00 GMT</pubDate></item><item><title>Declaration of partial email bankruptcy</title><link>https://lucaswerkmeister.de/posts/2024/11/26/email-bankruptcy/</link><description>&lt;article&gt;

&lt;p&gt;
        I hereby declare partial &lt;a href="https://en.wikipedia.org/wiki/Email_bankruptcy"&gt;email bankruptcy&lt;/a&gt;.
      &lt;/p&gt;
&lt;p&gt;
        Due to an unfortunate accident on my mail server,
        I have lost track of all my unread emails
        (if I still have them, they’re marked as read now),
        and I have probably also completely lost an unknown amount of emails in my inbox.
      &lt;/p&gt;
&lt;p&gt;
        If you were waiting for anything from me,
        please let me know and I’ll try to pick it up again.
        (Although you are very much welcome, in fact encouraged,
        to wait a bit before doing so!
        I’m not going to be able to immediately react to
        everything that was waiting for a response from me.)
        This is not strictly limited to things you directly sent to me via email,
        as some of the unread emails were probably notifications from various other platforms.
      &lt;/p&gt;

&lt;/article&gt;</description><guid isPermaLink="true">https://lucaswerkmeister.de/posts/2024/11/26/email-bankruptcy/</guid><pubDate>Tue, 26 Nov 2024 00:00:00 GMT</pubDate></item><item><title>How To Invent Everything review</title><link>https://lucaswerkmeister.de/posts/2024/11/23/how-to-invent-everything-review/</link><description>&lt;article&gt;

&lt;div&gt;
&lt;p&gt;
&lt;span&gt;

 &lt;!-- no work/edition distinction in RDFa apparently 🤷 --&gt;

&lt;i&gt;How To Invent Everything: A Survival Guide for the Stranded Time Traveler&lt;/i&gt;, &lt;!-- no title/subtitle distinction either --&gt;
            by
            &lt;span&gt;

&lt;span&gt;Ryan North&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;,
          is a fun book.
          The basic premise is that this is a guide for rebuilding civilization from scratch,
          if you should find yourself stranded in the past with nothing but this book.
        &lt;/p&gt;
&lt;p&gt;
          More specifically, the conceit of the entire book is that it’s &lt;em&gt;actually&lt;/em&gt; a guide for a stranded time traveler,
          and all of us reading the book in the 21st century aren’t really its intended target audience.
          In the introductory “note to readers”,
          the real author, Ryan North,
          claims to have discovered a copy of the text embedded in bedrock billions of years old,
          where it was presumably left by a real time traveler
          (who, being stranded on a very early Earth,
          would sadly have had little opportunity to make use of the guide);
          the bulk of the book (everything except said introductory note and the endnotes and index)
          was supposedly written by a fictional author (coincidentally &lt;em&gt;also&lt;/em&gt; named Ryan North),
          who lives in a future version of our own world which possesses time travel technology.
        &lt;/p&gt;
&lt;p&gt;
          I confess that at first I found this irritating.
          Why am I being asked, I thought, to believe (or suspend disbelief for) this ridiculous and pointless framing device?
          But as I kept reading, I came to appreciate it, and understood that it’s not pointless at all.
          By writing from the perspective of a fictional future author from a society with access to time machines,
          the real author gets to state all sorts of things with confidence which in reality we can only surmise or guess at,
          and then slap on an endnote along the lines of
          “my research has around two thousand years’ wiggle room on the date” or
          “all the research I found suggests the date here is at best an educated guess”.
          (And occasionally the fictional author can just directly describe a fun “temporal experiment”
          and leave it to the endnotes to point out that obviously we can’t really know
          what would happen if you dropped a functional wooden glider in Europe in 1000 CE.)
          This greatly streamlines the text and makes for a book that’s simply more fun to read.
        &lt;/p&gt;
&lt;p&gt;
          The actual guide part of the book is consistently interesting and entertaining.
          The explanations or instructions are often by necessity somewhat terse –
          it took me a while to understand what the point of the Bessemer process is,
          and I’m &lt;em&gt;still&lt;/em&gt; not sure how the various parts of a spinning wheel work without running into each other –
          but the writing style generally leaves me with the impression that I’d be able to figure things out somehow,
          and eager to try it out.
        &lt;/p&gt;
&lt;p&gt;
          A recurring theme of the book is how embarrassing the actual course of history and technological development is for humanity as a whole.
          This most often refers to the sheer amount of time humans needed to figure things out when they had all the necessary prerequisites
          (five fundamental technologies, all of which could have been invented at any point in humanity’s history,
          are described in “a table any human should be embarrassed to even be in the same room with”;
          on “non-sucky numbers”, the author remarks that
          “&lt;i&gt;Homo sapiens sapiens&lt;/i&gt;, a species that considers itself so smart that it put ‘smart’ in its own name,
          &lt;em&gt;twice&lt;/em&gt;, and in &lt;em&gt;Latin&lt;/em&gt;, took more than 40,000 years to figure [them] out”),
          but also to various inventors’ incorrect understanding of their inventions
          (on heavier-than-air flight: “for decades, every single person who flew managed to do so
          without having a correct understanding of how and why their planes really worked”).
          This makes for an amusing text
          (unless you want to get offended on the dead people’s behalf, I suppose)
          and also helps to reinforce the impression that,
          if you were trapped in the past with no supporting technology except this book,
          you &lt;em&gt;would&lt;/em&gt; be able to reinvent everything much faster than we managed in our real timeline.
        &lt;/p&gt;
&lt;p&gt;
          This also makes for an (I think) interesting comparison with the &lt;i&gt;Civilization&lt;/i&gt; games –
          when I first read the book, this wasn’t on my mind because it had been years since I’d played any,
          but on my more recent reread (having played a bunch of Civ6 in the meantime) I started to think about it.
          There are some superficial similarities between the two:
          both like to introduce technologies with vaguely related quotations, for example,
          and both feature a tech tree (Appendix A in the book).
          But the overall goal is quite different.
          The Civ tech tree is carefully balanced on gameplay grounds,
          and the game tries to approximate the course of civilization development in our actual history.
          Flight, for instance, has 21 prerequisite technologies (from animal husbandry to industrialization),
          and no matter what you do, you’re not going to get it much earlier than the modern era.
          &lt;i&gt;How To Invent Everything&lt;/i&gt;, on the other hand,
          delights in pointing out that lighter-than-air flight (i.e., hot-air balloons)
          requires nothing other than fabric and fire
          and could have been invented thousands of years ago,
          if only humans hadn’t been so insistent in trying to copy birds.
          In some cases, the book completely skips over a technological development in our timeline
          in favor of an alternative solution that is both simpler and better –
          for example, instead of teaching you how to invent (massively complicated) marine chronometers,
          it offers a totally different solution to the &lt;a href="https://en.wikipedia.org/wiki/Longitude_problem"&gt;longitude problem&lt;/a&gt;
          (which I’m not going to spoil here 😛).
        &lt;/p&gt;
&lt;p&gt;
          With all that said, I want to turn to my one major issue with the book.
          As you may have gathered at this point, the book has a focus on technology:
          there are sections on other subjects,
          such as nutrition (section 9), medicine (section 14), music (section 16),
          and a minimum of lip service is even paid to philosophy (section 12),
          but the bulk of the book is taken up by section 10,
          which lists various technologies, their prerequisites,
          and how to invent them ahead of schedule.
          For a popular science book aimed at geeks in the present day,
          this is totally fine and sensible.
        &lt;/p&gt;
&lt;p&gt;
          However, if you take the book’s conceit seriously,
          and actually consider it as a manual for rebuilding civilization from scratch,
          then I think the implications of this are frankly terrifying.
          The book takes pride in being, on account of all the knowledge inside it,
          “the most dangerous item on the planet” (section 3.2),
          and yet its approach to managing that danger,
          and ensuring that the knowledge is not misused,
          amounts to simply assuming that the reader is a decent person
          and hoping that this will result in a decent society with no major problems.
          For instance, in section 10.12.1, the author asserts that the reader
          “[won’t] have to labor under the hangover of thousands of years of patriarchy”;
          even if you assume that the reader didn’t end up in a time period with an existing patriarchal society
          (despite this being considered a possibility elsewhere in the book),
          it strikes me as bold to assume that the reader won’t have any biases of their own, conscious or otherwise.
        &lt;/p&gt;
&lt;p&gt;
          Topics which the book feels no need to even mention include:
          human rights and civil rights;
          democracy, representative democracy, and elected rather than hereditary heads of state (is this too much to ask of a Canadian author?);
          how to run democratic elections (universal and equal suffrage, secret ballots, the right to stand for office, etc.);
          constitutions, separation of powers, separation of church and state;
          the rule of law, the state monopoly on violence, basic legal principles like the right to counsel;
          why corporal punishment or the death penalty are bad ideas;
          the paradox of tolerance.
        &lt;/p&gt;
&lt;p&gt;
          I can think of some reasons not to include these things in the book, but none of them convince me.
          Sure, there’s limited space available,
          but surely a summary of the Universal Declaration of Human Rights would be more valuable
          than three full pages of frequencies for musical notes (Appendix G).
          Some of these things might seem obvious,
          but the book doesn’t otherwise shy away from repeating obvious things
          (if only to clown on humans for taking such a long time to figure them out anyway) –
          section 10.8.2 spends a page and a half on the concept of buttons (the clothes kind).
          And in any case, many of these ideas are far from obvious –
          they took society centuries and millennia to work out in our timeline.
          A book which lets the reader “fast forward” through technological progress
          should aim to put them on at least equal footing in terms of societal progress.
        &lt;/p&gt;
&lt;p&gt;
          Admittedly, some of these principles are harder to introduce to a civilization than technologies.
          To introduce glass, you “just” have to invent it and then you can expect
          that people will see that it’s useful and be interested in making more themselves;
          to introduce social principles, you have to explain them to people and convince them of them.
          But this should be all the more reason for a guide to civilization to prepare the reader with this information,
          so that they can actually try to convince their fellow humans using the arguments from the guide,
          rather than being left with nothing but a vague feeling of
          “well I think the society I’m from used to follow this principle but I’m not even sure why let alone how to convince others of it”.
        &lt;/p&gt;
&lt;p&gt;
          But as I said: this criticism mainly applies within the book’s fictional framework
          of being an actual guide for real time-travelers.
          For a reader in the real world, I’m still happy to recommend the book 🙂
        &lt;/p&gt;
&lt;/div&gt;

&lt;/article&gt;</description><guid isPermaLink="true">https://lucaswerkmeister.de/posts/2024/11/23/how-to-invent-everything-review/</guid><pubDate>Sat, 23 Nov 2024 00:00:00 GMT</pubDate></item><item><title>JBL Quantum 810 wireless headset review</title><link>https://lucaswerkmeister.de/posts/2024/09/28/jbl-quantum-810-review/</link><description>&lt;article&gt;

&lt;p&gt;
        Quick review of the
        &lt;span&gt;

&lt;span&gt;
            JBL Quantum 810 wireless headset&lt;/span&gt;&lt;/span&gt;,
        which I recently got.
        My main point of comparison is my previous headset (also wireless),
        a HyperX Cloud Flight, whose charging port eventually broke down.
      &lt;/p&gt;
&lt;div&gt;
&lt;p&gt;
          I’ll start with some general characteristics.
          I wasn’t dead set on an exact choice of headset model,
          but I wanted it to have these features
          (and then made my choice by looking around the store website,
          checking a few reviews,
          and then seeing what was in stock on site and in a sensible price range):
        &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
            A headset, i.e. headphones with an integrated microphone.
            I don’t want to subject the people I talk with to terrible webcam audio,
            and I don’t want to have a separate microphone with a mic stand on my desk either.
          &lt;/li&gt;
&lt;li&gt;
            Wireless.
            I originally got a wireless headset to replace my old wired one early in the pandemic,
            because coworkers were complaining about echo from me,
            which I attributed to crosstalk between the speaker and microphone wires.
            But it turns out a wireless headset (or headphones) is also massively convenient –
            it’s just nice to be able to walk around the apartment with it,
            or even just lean further back in my chair than I previously could.
            I’d recommend it for anyone.
          &lt;/li&gt;
&lt;li&gt;
            With a USB dongle.
            This is quite important for me because it means I can plug it into my USB switch,
            together with the mouse, keyboard, and webcam,
            and then easily toggle all four peripherals between my work laptop and my private PC.
            (The screens are on separate switches.
            You can get devices that let you switch everything together –
            the usual term seems to be “KVM” –
            but they appear to be much more expensive,
            so I’m happy to just press three buttons instead of one.)
            The JBL Quantum 810 apparently also supports bluetooth,
            but I haven’t had a reason to try that so far.
            (It can also continue working while plugged in to charge,
            which wasn’t the case for the HyperX Cloud Flight.)
          &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
          And some specific points on the JBL Quantum 810:
        &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
            Its microphone is attached permanently,
            with a hinge to let you move it between your mouth (in use) and your temple (not in use).
            The microphone is automatically muted while pointed upwards,
            which is convenient, though I wouldn’t rely on it alone.
            The HyperX Cloud Flight had a detachable microphone, which wasn’t bad,
            but it certainly was a bit more fiddly to put it on my head and then attach the microphone each time.
          &lt;/li&gt;
&lt;li&gt;
            The power switch is a switch (with on and off positions),
            rather than a button you have to hold down for several seconds.
            I find this much more convenient.
            (It more than outweighs the minor disadvantage that,
            if the headset turns itself off due to inactivity or low battery,
            the button’s position no longer reflects the actual power state.)
          &lt;/li&gt;
&lt;li&gt;
            The wireless range is longer than with my previous headset –
            enough to pretty reliably reach my whole apartment.
            (Which isn’t huge, but the HyperX Cloud Flight only reached part of it.)
            I assume the previous headset used Bluetooth internally, or at least the 2.4 GHz frequency range;
            the JBL Quantum 810, when used with its dongle, apparently uses the 5 GHz frequency range,
            and I assume that helps with the range.
            (If you want measurements, my guess would be that the range is at least 10 m,
            probably closer to 15 m, and that’s with a concrete wall or two in the way.)
          &lt;/li&gt;
&lt;li&gt;
            It works on Linux, at least using recent versions.
            (I found some reports of issues around 2021 or so,
            so if you’re on an older kernel you might want to look that up for yourself.)
            It shows up as a sound output and a separate input for the microphone, as you’d expect.
            Unlike with the HyperX Cloud Flight,
            the volume wheel on the headset does &lt;em&gt;not&lt;/em&gt; affect the system volume – they’re separate.
            Initially I had the wheel set to the max and the system volume around 50%,
            but it turned out that this meant my system audio was very quiet on live streams
            (i.e., my viewers couldn’t hear the game I was playing),
            so now I instead have my system volume at max and the wheel somewhere around the middle.
            I would prefer the wheel to adjust the system volume instead,
            but I can live with this as well.
          &lt;/li&gt;
&lt;li&gt;
            The JBL Quantum 810 has some features I’m not interested in –
            I don’t remember the exact details,
            but there was at least some form of RGB lighting,
            and some kind of noise cancelling.
            It was possible (and, following the manual, relatively straightforward)
            to turn all of these off using the controls on the headset itself:
            I did &lt;em&gt;not&lt;/em&gt; need some vendor-specific Windows-only app,
            and I appreciate that.
          &lt;/li&gt;
&lt;li&gt;
            Speaking of unused features:
            the JBL Quantum 810 has a second “volume” wheel above the “real” one.
            Apparently this is meant to let gamers adjust the relative volume of the game vs. voice chat –
            if I understood correctly,
            the headset is supposed to offer two sound outputs,
            you configure your system to send the game to one and the voice chat to the other,
            and then the wheel on the headset adjusts the mixing between the two.
            I haven’t seen any trace of this on Linux,
            but I also don’t need it anyway,
            so I just keep that wheel turned all the way to the “game” end.
            🤷
          &lt;/li&gt;
&lt;li&gt;
            The battery life is quite remarkable compared to the HyperX Cloud Flight.
            I haven’t kept close track of it,
            but so far my guess would be that I’m recharging it slightly less than once per week,
            and I would say I don’t exactly use it sparingly.
            (2025-02-10 update: I seem to be recharging it every ten days or so.)
            The manufacturer claims a battery life of 43 hours
            (when using the dongle and with RGB turned off;
            over Bluetooth it would apparently be 49 hours?),
            and I have no reason to disbelieve that so far.
            It also charges over USB-C, which is convenient at this point in my life
            (the HyperX Cloud Flight charges over Micro-USB,
            but I have barely any other devices left which require that plug).
          &lt;/li&gt;
&lt;li&gt;
            The biggest downside:
            when it’s not playing sound, the headset likes to turn its speakers off.
            (I’m guessing this helps with the battery life?)
            This sounds minor, but it gets pretty annoying after a while.
            It means that, if I’m wearing the headset but not currently listening to anything,
            any quick notification chime will either sound like it has a weird fade-in effect for no reason,
            or I won’t even hear it at all.
            And the headset’s threshold for “not playing sound” is high enough
            that sometimes it turns itself off while I &lt;em&gt;am&lt;/em&gt; listening to some quiet audio,
            and I’ll have to crank up the volume just to stop the headset from turning itself off,
            even if my ears could hear the audio well enough at the previous volume.
            I searched around a bit,
            but it doesn’t seem like there’s anything you can do about this behavior.
          &lt;/li&gt;
&lt;li&gt;
            And a less important downside for the end:
            while the battery life is good,
            there’s no way to see the current battery level,
            and the first of the three (IIRC) “battery low” chimes comes a bit late for my taste –
            only an hour or two, if memory serves, before the headset is totally dead.
            (2025-02-10 update: three are three “battery low” chimes in total,
            and today I got them all within 30-40 minutes,
            and then the battery was totally dead within 15-30 minutes of the last chime.)
            I wouldn’t mind having a bit more advance notice that I’ll have to charge the headset soon.
            That said, the headset recharges pretty quickly,
            so it doesn’t take too long to get it back into a usable state.
          &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
          I think that’s all I have to say about it right now!
          (If you were hoping for me to say something about the audio quality…
          sorry, I don’t really have an insightful comment on that.
          It sounds alright.)
        &lt;/p&gt;
&lt;/div&gt;

&lt;/article&gt;</description><guid isPermaLink="true">https://lucaswerkmeister.de/posts/2024/09/28/jbl-quantum-810-review/</guid><pubDate>Sat, 28 Sep 2024 00:00:00 GMT</pubDate></item><item><title>Why MediaWiki permanent links aren’t fully permanent</title><link>https://lucaswerkmeister.de/posts/2024/08/14/mediawiki-permalinks/</link><description>&lt;article&gt;

&lt;p&gt;
&lt;a href="https://en.wikipedia.org/wiki/MediaWiki"&gt;MediaWiki&lt;/a&gt;,
        the software behind &lt;a href="https://en.wikipedia.org/wiki/Wikipedia"&gt;Wikipedia&lt;/a&gt;
        and &lt;a href="https://wikiapiary.com/wiki/Main_Page"&gt;many other wikis&lt;/a&gt;,
        allows visitors to copy a “permanent link” (or “permalink”) of the article they are currently viewing.
      &lt;/p&gt;
&lt;p&gt;
        As &lt;a href="https://en.wikipedia.org/wiki/Help:Permanent_link"&gt;English Wikipedia’s help page on permanent links&lt;/a&gt; notes,
        these links aren’t fully “permanent”:
        visiting these links later is not guaranteed to show the exact same content.
        I thought it would be useful to list some of the different ways in which differences can appear.
      &lt;/p&gt;
&lt;aside&gt;
&lt;p&gt;
          Please note that I’m mainly focusing on Wikimedia wikis here,
          and for examples will often refer to Wikipedias in particular;
          many of these issues will also affect other MediaWiki sites,
          but there are probably additional ways in which third-party wikis can have permalinks’ contents change
          (e.g. Fandom / Wikia has tons of custom extensions),
          and I’m generally not super interested in those.
          (But if I forgot something relevant to Wikimedia or standard MediaWiki,
          let me know and I might edit it in.)
        &lt;/p&gt;
&lt;/aside&gt;
&lt;h2&gt;What a permalink is&lt;/h2&gt;
&lt;p&gt;
        While the permalink shown in the sidebar (or “tools” menu) contains both the &lt;code&gt;title=&lt;/code&gt; and &lt;code&gt;oldid=&lt;/code&gt; URL parameters
        (example: &lt;a href="https://en.wikipedia.org/w/index.php?title=Wikimedia_Commons&amp;amp;oldid=1185477778"&gt;https://en.wikipedia.org/w/index.php?title=Wikimedia_Commons&amp;amp;oldid=1185477778&lt;/a&gt;),
        only the &lt;code&gt;oldid=&lt;/code&gt; is actually required
        (equivalent example: &lt;a href="https://en.wikipedia.org/w/index.php?oldid=1185477778"&gt;https://en.wikipedia.org/w/index.php?oldid=1185477778&lt;/a&gt;).
        The value of this parameter is the revision ID of a page,
        and it tells MediaWiki to use the content of this revision of that page instead of its latest revision.
        For normal wiki articles, this content is the wikitext (the source code of the article),
        which is then rendered and shown to the visitor;
        you can also see the content unrendered by adding &lt;code&gt;&amp;amp;action=raw&lt;/code&gt; to the URL
        (&lt;a href="https://en.wikipedia.org/w/index.php?oldid=1185477778&amp;amp;action=raw"&gt;example&lt;/a&gt;).
      &lt;/p&gt;
&lt;h2&gt;Changes in on-wiki content&lt;/h2&gt;
&lt;p&gt;
        Let’s start with the biggest one.
        Visiting a permalink only loads the content of the page itself as of the revision specified in the URL.
        Any other &lt;a href="https://en.wikipedia.org/wiki/Help:Template"&gt;templates&lt;/a&gt;,
        &lt;a href="https://en.wikipedia.org/wiki/Wikipedia:Lua"&gt;Lua/Scribunto modules&lt;/a&gt;,
        or &lt;a href="https://en.wikipedia.org/wiki/Help:Transclusion"&gt;other transcluded pages&lt;/a&gt;
        are loaded, parsed / evaluated and shown using their latest revision,
        not whatever was their latest revision when the permalink’s revision was created.
        Templates may look and behave very differently from what they used to do;
        their parameters also may or may not be compatible with the template invocation in the old revision,
        depending on how the community edited the templates.
      &lt;/p&gt;
&lt;p&gt;
        In fact, it’s not even guaranteed that the page will show the same templates at all.
        Templates are looked up by their name according to the old revision’s wikitext,
        but templates can be deleted, recreated, or renamed.
        For instance, several wikis now have templates like &lt;code&gt;{{Q|Q123}}&lt;/code&gt; and/or &lt;code&gt;{{P|P123}}&lt;/code&gt;
        to show Wikidata items and/or properties;
        but in the past, “&lt;code&gt;Q&lt;/code&gt;” or “&lt;code&gt;P&lt;/code&gt;” may have referred to different templates that were later deleted,
        as e.g. on &lt;a href="https://eo.wikipedia.org/wiki/Specialaĵo:Protokolo?page=Ŝablono%3AP"&gt;Esperanto&lt;/a&gt;,
        &lt;a href="https://zh.wikipedia.org/wiki/Special:%E6%97%A5%E5%BF%97?page=Template%3AP"&gt;Chinese&lt;/a&gt; or
        &lt;a href="https://es.wikipedia.org/wiki/Especial:Registro?page=Plantilla%3AQ"&gt;Spanish&lt;/a&gt; Wikipedia.
        (These are conveniently short names, after all!
        On English Wikipedia, &lt;code&gt;{{&lt;a href="https://en.wikipedia.org/wiki/Template:P"&gt;P&lt;/a&gt;}}&lt;/code&gt; is still a smiley 🙂)
      &lt;/p&gt;
&lt;p&gt;
        A more niche phenomenon is redlinks (h/t &lt;a href="https://mamot.fr/@pintoch/112953410745774438"&gt;Antonin Delpeuch&lt;/a&gt; for this one).
        MediaWiki shows internal links to existing pages in blue, and links to pages that don’t exist in red;
        but when rendering a permalink, this refers to whether the page currently exists,
        not whether it used to exist when the old revision was saved.
        (You can actually see this in the &lt;a href="https://en.wikipedia.org/w/index.php?oldid=1185477778"&gt;example permalink from earlier&lt;/a&gt;,
        where a “not to be confused with” &lt;a href="https://en.wikipedia.org/wiki/Wikipedia:Hatnote"&gt;hatnote&lt;/a&gt; points to a redlink that was evidently deleted in the meantime.)
      &lt;/p&gt;
&lt;p&gt;
        Articles may also be affected by changes in the wiki’s default interface &lt;a href="https://en.wikipedia.org/wiki/Wikipedia:Common.js_and_common.css"&gt;JavaScript and CSS&lt;/a&gt;
        and default &lt;a href="https://en.wikipedia.org/wiki/Wikipedia:Gadget"&gt;gadgets&lt;/a&gt;,
        which may interact with page contents.
        For example, styles for frequently used templates are sometimes moved into &lt;code&gt;common.css&lt;/code&gt; (or out of it),
        and some templates may rely on gadgets for interactive functionality
        (like – CW for some medical imagery in the next link – Wikimedia Commons’ &lt;a href="https://commons.wikimedia.org/wiki/Template:Imagestack"&gt;Imagestack&lt;/a&gt;).
        As with templates, it’s ultimately up to the community whether changes here are backwards compatible or not.
      &lt;/p&gt;
&lt;h2&gt;Changes in content on other wikis&lt;/h2&gt;
&lt;p&gt;
        Articles can also use content from other wikis,
        the most prominent example being Wikimedia Commons images
        (which, thanks to &lt;a href="https://www.mediawiki.org/wiki/InstantCommons"&gt;InstantCommons&lt;/a&gt;,
        are used not only on Wikimedia wikis but also &lt;em&gt;many&lt;/em&gt; other wikis using MediaWiki).
        A permalink will show the latest version of an image on Commons,
        which is not necessarily the same version as was shown when the old revision was saved –
        although, due to Commons’ &lt;a href="https://commons.wikimedia.org/w/index.php?title=Commons:Don%27t_be_bold&amp;amp;rdfrom=commons:Commons:Be_bold"&gt;don’t be bold&lt;/a&gt; policy,
        the differences should usually be minor
        (e.g. a higher-quality version or a slightly improved crop).
      &lt;/p&gt;
&lt;p&gt;
        The image on Commons may also have been deleted in the meantime,
        e.g. because it turned out to be a copyright violation.
        In this case, the permalink will show the image as a redlink.
      &lt;/p&gt;
&lt;p&gt;
        There are also other ways for wikis to refer to Commons.
        Prior to its &lt;a href="https://phabricator.wikimedia.org/T334940"&gt;undeployment due to security issues&lt;/a&gt;,
        the Graph extension could load data from the Data: namespace on Commons,
        and show it e.g. as a line chart.
      &lt;/p&gt;
&lt;p&gt;
        And then there’s &lt;a href="https://en.wikipedia.org/wiki/Wikidata"&gt;Wikidata&lt;/a&gt;.
        Wikipedia editors can, at their discretion, make an article read information from Wikidata;
        this has a number of benefits,
        but is also another case where visible parts of an article aren’t part of the article’s source code
        and viewing old revisions will still pull the latest version from the referenced place.
      &lt;/p&gt;
&lt;h2&gt;Changes in the software&lt;/h2&gt;
&lt;p&gt;
        Finally, the software which actually renders the old revision’s content is also subject to changes.
        MediaWiki sees roughly two thousand code changes per release, &lt;!-- git log --oneline --grep='Update git submodules\|Localisation updates\|^Merge' --invert-grep gerrit/REL1_41..gerrit/REL1_42 | wc -l # repeat for a few older REL branch pairs --&gt;
        and any of them might affect the way an article looks.
        While the parser is developed fairly conservatively
        (as nobody wants to break millions of existing pages all at once),
        there are sometimes breaking changes to it;
        many of them may be preceded by on-wiki fixes to avoid the breakage
        (e.g. using the &lt;a href="https://en.wikipedia.org/wiki/Wikipedia:Linter"&gt;Linter&lt;/a&gt; to locate problematic constructs whose behavior will change in future),
        but this doesn’t help when looking at old revisions.
      &lt;/p&gt;
&lt;aside&gt;
&lt;p&gt;
          (Side note: Because the parser always produces &lt;em&gt;some&lt;/em&gt; output HTML,
          and never returns an error like “invalid input wikitext”,
          arguably any change to its output is a breaking change.
          After all, even if some new wikitext syntax is intentionally introduced,
          that syntax will previously have rendered in a different way,
          and it’s theoretically possible that someone used that syntax and relies on its previous behavior.
          This is also something that irritates me about Markdown “flavors”
          that describe themselves as “compatible with” or “strict supersets of” CommonMark.)
        &lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;
        Also, similar to the earlier point about the wiki’s default CSS and JS,
        the old revision may also have relied on CSS or JS included in MediaWiki, which is subject to change.
        For example, many articles rely on &lt;a href="https://www.mediawiki.org/wiki/Manual:Collapsible_elements"&gt;collapsible elements&lt;/a&gt;
        (which is an intentional feature offered by MediaWiki which has stayed very stable so far),
        and many pages (help and project pages more so than articles, I believe)
        rely or relied on styles from the OOUI or MediaWiki UI interface libraries,
        a practice that is &lt;a href="https://phabricator.wikimedia.org/T360010"&gt;increasingly discouraged&lt;/a&gt;
        as these libraries are being phased out in favor of Codex
        (though &lt;a href="https://phabricator.wikimedia.org/T355242"&gt;the replacement is not yet clear&lt;/a&gt;).
      &lt;/p&gt;
&lt;h2&gt;Now what?&lt;/h2&gt;
&lt;p&gt;
        If you want to create a new link to some wiki content as you currently see it:
        you can use a permalink as offered by MediaWiki (it’s a pretty “lightweight” solution),
        but if you want to be absolutely sure that everyone else will see the same content,
        I believe the only way to do that and avoid all the issues here
        is to grab a snapshot of the &lt;em&gt;rendered&lt;/em&gt; wiki content and distribute that.
        You can do this via a trusted intermediary,
        such as the &lt;a href="https://en.wikipedia.org/wiki/Internet_Archive"&gt;Internet Archive&lt;/a&gt;’s &lt;a href="https://en.wikipedia.org/wiki/Wayback_Machine"&gt;Wayback Machine&lt;/a&gt;,
        or you can save the page yourself.
      &lt;/p&gt;
&lt;p&gt;
        If you’re looking at a permalink that you found somewhere else,
        I think it’s worth keeping in mind that there are some caveats to it,
        but 99% of the time it’s fine –
        in practice, I think most of the issues listed here are more theoretical than practical concerns.
        If you really want to be sure you’re seeing a page as it originally appeared,
        you can try to find a snapshot on the Wayback Machine or another web archiving site.
      &lt;/p&gt;
&lt;p&gt;
        There is also a proposal for a &lt;a href="https://www.mediawiki.org/wiki/User:Ainali/Wiki_Timeslicer"&gt;Wiki Timeslicer&lt;/a&gt; tool which would bypass some of these problems.
        (Personally, I’m skeptical how feasible it is, to be honest.
        But it probably is possible to improve on MediaWiki’s own functionality, at least.)
      &lt;/p&gt;

&lt;/article&gt;</description><guid isPermaLink="true">https://lucaswerkmeister.de/posts/2024/08/14/mediawiki-permalinks/</guid><pubDate>Wed, 14 Aug 2024 00:00:00 GMT</pubDate></item></channel></rss>
